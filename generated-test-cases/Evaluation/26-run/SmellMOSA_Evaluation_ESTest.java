/*
 * This file was automatically generated by EvoSuite
 * Thu Dec 17 08:39:17 GMT 2015
 */

package weka.classifiers;

import static org.junit.Assert.*;
import org.junit.Test;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.testdata.EvoSuiteFile;
import org.evosuite.runtime.testdata.EvoSuiteLocalAddress;
import org.evosuite.runtime.testdata.EvoSuiteRemoteAddress;
import org.evosuite.runtime.testdata.EvoSuiteURL;
import org.junit.runner.RunWith;
import weka.classifiers.Classifier;
import weka.classifiers.CostMatrix;
import weka.classifiers.Evaluation;
import weka.classifiers.Sourcable;
import weka.classifiers.functions.SGD;
import weka.classifiers.functions.SGDText;
import weka.classifiers.meta.AdditiveRegression;
import weka.classifiers.meta.Bagging;
import weka.classifiers.meta.CostSensitiveClassifier;
import weka.classifiers.rules.OneR;
import weka.classifiers.rules.PART;
import weka.classifiers.trees.J48;
import weka.core.BinarySparseInstance;
import weka.core.Capabilities;
import weka.core.DenseInstance;
import weka.core.FastVector;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.SparseInstance;
import weka.core.TestInstances;
import weka.core.converters.ConverterUtils;
import weka.core.converters.DatabaseLoader;
import weka.core.converters.Loader;
import weka.core.converters.SerializedInstancesLoader;
import weka.core.converters.TextDirectoryLoader;
import weka.core.neighboursearch.CoverTree;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true) 
public class Evaluation_ESTest extends Evaluation_ESTest_scaffolding {

  //Test case number: 0
  /*
   * 1 covered goal:
   * Goal 1. weka.classifiers.Evaluation.evaluationForSingleInstance(Lweka/classifiers/Classifier;Lweka/core/Instance;Z)D: I19 Branch 181 IFEQ L1726 - true
   */

  @Test
  public void test00()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertNotNull(textDirectoryLoader0);
      
      Instances instances0 = textDirectoryLoader0.getDataSet();
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertNotNull(evaluation0);
      
      double[] doubleArray0 = new double[1];
      PART pART0 = new PART();
      assertEquals("The confidence factor used for pruning (smaller values incur more pruning).", pART0.confidenceFactorTipText());
      assertEquals("The minimum number of instances per rule.", pART0.minNumObjTipText());
      assertEquals("The seed used for randomizing the data when reduced-error pruning is used.", pART0.seedTipText());
      assertFalse(pART0.getUnpruned());
      assertFalse(pART0.getReducedErrorPruning());
      assertEquals("Whether pruning is performed.", pART0.unprunedTipText());
      assertEquals("Determines the amount of data used for reduced-error pruning.  One fold is used for pruning, the rest for growing the rules.", pART0.numFoldsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", pART0.debugTipText());
      assertEquals("Whether MDL correction is used when finding splits on numeric attributes.", pART0.useMDLcorrectionTipText());
      assertEquals(3, pART0.getNumFolds());
      assertEquals("Whether reduced-error pruning is used instead of C.4.5 pruning.", pART0.reducedErrorPruningTipText());
      assertEquals("Whether to use binary splits on nominal attributes when building the partial trees.", pART0.binarySplitsTipText());
      assertEquals(2, pART0.getMinNumObj());
      assertTrue(pART0.getUseMDLcorrection());
      assertFalse(pART0.getDebug());
      assertEquals(0.25F, pART0.getConfidenceFactor(), 0.01F);
      assertEquals(1, pART0.getSeed());
      assertFalse(pART0.getBinarySplits());
      assertNotNull(pART0);
      
      int[] intArray0 = new int[2];
      SparseInstance sparseInstance0 = new SparseInstance((-3453.562995721), doubleArray0, intArray0, 3643);
      assertEquals(6, sparseInstance0.s_numericAfterDecimalPoint);
      assertEquals(0, sparseInstance0.numValues());
      assertEquals((-3453.562995721), sparseInstance0.weight(), 0.01D);
      assertEquals(3643, sparseInstance0.numAttributes());
      assertArrayEquals(new int[] {0, 0}, intArray0);
      assertArrayEquals(new double[] {0.0}, doubleArray0, 0.01);
      assertNotNull(sparseInstance0);
      
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((Instance) sparseInstance0);
      assertEquals(6, binarySparseInstance0.s_numericAfterDecimalPoint);
      assertEquals(6, sparseInstance0.s_numericAfterDecimalPoint);
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals((-3453.562995721), binarySparseInstance0.weight(), 0.01D);
      assertEquals(3643, binarySparseInstance0.numAttributes());
      assertEquals(0, sparseInstance0.numValues());
      assertEquals((-3453.562995721), sparseInstance0.weight(), 0.01D);
      assertEquals(3643, sparseInstance0.numAttributes());
      assertArrayEquals(new int[] {0, 0}, intArray0);
      assertArrayEquals(new double[] {0.0}, doubleArray0, 0.01);
      assertNotNull(binarySparseInstance0);
      
      try {
        double double0 = evaluation0.evaluationForSingleInstance(pART0, binarySparseInstance0, false);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
      }
  }

  //Test case number: 1
  /*
   * 1 covered goal:
   * Goal 1. weka.classifiers.Evaluation.makeOptionString(Lweka/classifiers/Classifier;Z)Ljava/lang/String;: I380 Branch 351 IFEQ L3567 - false
   */

  @Test
  public void test01()  throws Throwable  {
      J48 j48_0 = new J48();
      assertEquals("Whether parts are removed that do not reduce training error.", j48_0.collapseTreeTipText());
      assertEquals(3, j48_0.getNumFolds());
      assertFalse(j48_0.getBinarySplits());
      assertEquals(0.25F, j48_0.getConfidenceFactor(), 0.01F);
      assertFalse(j48_0.getUseLaplace());
      assertEquals("Determines the amount of data used for reduced-error pruning.  One fold is used for pruning, the rest for growing the tree.", j48_0.numFoldsTipText());
      assertFalse(j48_0.getDebug());
      assertTrue(j48_0.getSubtreeRaising());
      assertEquals("The minimum number of instances per leaf.", j48_0.minNumObjTipText());
      assertEquals(1, j48_0.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", j48_0.debugTipText());
      assertFalse(j48_0.getReducedErrorPruning());
      assertFalse(j48_0.getUnpruned());
      assertEquals(1, j48_0.graphType());
      assertEquals("Whether reduced-error pruning is used instead of C.4.5 pruning.", j48_0.reducedErrorPruningTipText());
      assertEquals("The confidence factor used for pruning (smaller values incur more pruning).", j48_0.confidenceFactorTipText());
      assertEquals("The seed used for randomizing the data when reduced-error pruning is used.", j48_0.seedTipText());
      assertEquals("Whether to consider the subtree raising operation when pruning.", j48_0.subtreeRaisingTipText());
      assertEquals(2, j48_0.getMinNumObj());
      assertEquals("Whether to use binary splits on nominal attributes when building the trees.", j48_0.binarySplitsTipText());
      assertTrue(j48_0.getCollapseTree());
      assertFalse(j48_0.getSaveInstanceData());
      assertTrue(j48_0.getUseMDLcorrection());
      assertEquals("Whether counts at leaves are smoothed based on Laplace.", j48_0.useLaplaceTipText());
      assertEquals("Whether MDL correction is used when finding splits on numeric attributes.", j48_0.useMDLcorrectionTipText());
      assertEquals("Whether to save the training data for visualization.", j48_0.saveInstanceDataTipText());
      assertEquals("Whether pruning is performed.", j48_0.unprunedTipText());
      assertNotNull(j48_0);
      
      String string0 = Evaluation.makeOptionString(j48_0, true);
      assertEquals("\n\nGeneral options:\n\n-h or -help\n\tOutput help information.\n-synopsis or -info\n\tOutput synopsis for classifier (use in conjunction  with -h)\n-t <name of training file>\n\tSets training file.\n-T <name of test file>\n\tSets test file. If missing, a cross-validation will be performed\n\ton the training data.\n-c <class index>\n\tSets index of class attribute (default: last).\n-x <number of folds>\n\tSets number of folds for cross-validation (default: 10).\n-no-cv\n\tDo not perform any cross validation.\n-split-percentage <percentage>\n\tSets the percentage for the train/test set split, e.g., 66.\n-preserve-order\n\tPreserves the order in the percentage split.\n-s <random number seed>\n\tSets random number seed for cross-validation or percentage split\n\t(default: 1).\n-m <name of file with cost matrix>\n\tSets file with cost matrix.\n-l <name of input file>\n\tSets model input file. In case the filename ends with '.xml',\n\ta PMML file is loaded or, if that fails, options are loaded\n\tfrom the XML file.\n-d <name of output file>\n\tSets model output file. In case the filename ends with '.xml',\n\tonly the options are saved to the XML file, not the model.\n-v\n\tOutputs no statistics for training data.\n-o\n\tOutputs statistics only, not the classifier.\n-i\n\tOutputs detailed information-retrieval statistics for each class.\n-k\n\tOutputs information-theoretic statistics.\n-classifications \"weka.classifiers.evaluation.output.prediction.AbstractOutput + options\"\n\tUses the specified class for generating the classification output.\n\tE.g.: weka.classifiers.evaluation.output.prediction.PlainText\n-p range\n\tOutputs predictions for test instances (or the train instances if\n\tno test instances provided and -no-cv is used), along with the \n\tattributes in the specified range (and nothing else). \n\tUse '-p 0' if no attributes are desired.\n\tDeprecated: use \"-classifications ...\" instead.\n-distribution\n\tOutputs the distribution instead of only the prediction\n\tin conjunction with the '-p' option (only nominal classes).\n\tDeprecated: use \"-classifications ...\" instead.\n-r\n\tOnly outputs cumulative margin distribution.\n-z <class name>\n\tOnly outputs the source representation of the classifier,\n\tgiving it the supplied name.\n-g\n\tOnly outputs the graph representation of the classifier.\n-xml filename | xml-string\n\tRetrieves the options from the XML-data instead of the command line.\n-threshold-file <file>\n\tThe file to save the threshold data to.\n\tThe format is determined by the extensions, e.g., '.arff' for ARFF \n\tformat or '.csv' for CSV.\n-threshold-label <label>\n\tThe class label to determine the threshold data for\n\t(default is the first label)\n\nOptions specific to weka.classifiers.trees.J48:\n\n-U\n\tUse unpruned tree.\n-O\n\tDo not collapse tree.\n-C <pruning confidence>\n\tSet confidence threshold for pruning.\n\t(default 0.25)\n-M <minimum number of instances>\n\tSet minimum number of instances per leaf.\n\t(default 2)\n-R\n\tUse reduced error pruning.\n-N <number of folds>\n\tSet number of folds for reduced error\n\tpruning. One fold is used as pruning set.\n\t(default 3)\n-B\n\tUse binary splits only.\n-S\n\tDon't perform subtree raising.\n-L\n\tDo not clean up after the tree has been built.\n-A\n\tLaplace smoothing for predicted probabilities.\n-J\n\tDo not use MDL correction for info gain on numeric attributes.\n-Q <seed>\n\tSeed for random data shuffling (default 1).\n\nSynopsis for weka.classifiers.trees.J48:\n\nClass for generating a pruned or unpruned C4.5 decision tree. For more information, see\n\nRoss Quinlan (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, San Mateo, CA.", string0);
      assertEquals("Whether parts are removed that do not reduce training error.", j48_0.collapseTreeTipText());
      assertEquals(3, j48_0.getNumFolds());
      assertFalse(j48_0.getBinarySplits());
      assertEquals(0.25F, j48_0.getConfidenceFactor(), 0.01F);
      assertFalse(j48_0.getUseLaplace());
      assertEquals("Determines the amount of data used for reduced-error pruning.  One fold is used for pruning, the rest for growing the tree.", j48_0.numFoldsTipText());
      assertFalse(j48_0.getDebug());
      assertTrue(j48_0.getSubtreeRaising());
      assertEquals("The minimum number of instances per leaf.", j48_0.minNumObjTipText());
      assertEquals(1, j48_0.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", j48_0.debugTipText());
      assertFalse(j48_0.getReducedErrorPruning());
      assertFalse(j48_0.getUnpruned());
      assertEquals(1, j48_0.graphType());
      assertEquals("Whether reduced-error pruning is used instead of C.4.5 pruning.", j48_0.reducedErrorPruningTipText());
      assertEquals("The confidence factor used for pruning (smaller values incur more pruning).", j48_0.confidenceFactorTipText());
      assertEquals("The seed used for randomizing the data when reduced-error pruning is used.", j48_0.seedTipText());
      assertEquals("Whether to consider the subtree raising operation when pruning.", j48_0.subtreeRaisingTipText());
      assertEquals(2, j48_0.getMinNumObj());
      assertEquals("Whether to use binary splits on nominal attributes when building the trees.", j48_0.binarySplitsTipText());
      assertTrue(j48_0.getCollapseTree());
      assertFalse(j48_0.getSaveInstanceData());
      assertTrue(j48_0.getUseMDLcorrection());
      assertEquals("Whether counts at leaves are smoothed based on Laplace.", j48_0.useLaplaceTipText());
      assertEquals("Whether MDL correction is used when finding splits on numeric attributes.", j48_0.useMDLcorrectionTipText());
      assertEquals("Whether to save the training data for visualization.", j48_0.saveInstanceDataTipText());
      assertEquals("Whether pruning is performed.", j48_0.unprunedTipText());
      assertNotNull(string0);
  }

  //Test case number: 2
  /*
   * 1 covered goal:
   * Goal 1. weka.classifiers.Evaluation.handleCostOption(Ljava/lang/String;I)Lweka/classifiers/CostMatrix;: I6 Branch 165 IFNULL L1574 - true
   */

  @Test
  public void test02()  throws Throwable  {
      CostMatrix costMatrix0 = Evaluation.handleCostOption((String) null, (-3));
      assertNull(costMatrix0);
  }

  //Test case number: 3
  /*
   * 2 covered goals:
   * Goal 1. weka.classifiers.Evaluation.KBMeanInformation()D: I7 Branch 208 IFNE L2335 - true
   * Goal 2. weka.classifiers.Evaluation.KBMeanInformation()D: I35 Branch 209 IFEQ L2339 - true
   */

  @Test
  public void test03()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertNotNull(textDirectoryLoader0);
      
      Instances instances0 = textDirectoryLoader0.getDataSet();
      assertEquals(0, instances0.numClasses());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numInstances());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertEquals(0, instances0.numClasses());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numInstances());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertNotNull(evaluation0);
      
      double double0 = evaluation0.KBMeanInformation();
      assertEquals(Double.NaN, double0, 0.01D);
      assertEquals(0, instances0.numClasses());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numInstances());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertFalse(textDirectoryLoader0.getDebug());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
  }

  //Test case number: 4
  /*
   * 81 covered goals:
   * Goal 1. weka.classifiers.Evaluation.weightedRecall()D: root-Branch
   * Goal 2. weka.classifiers.Evaluation.recall(I)D: root-Branch
   * Goal 3. weka.classifiers.Evaluation.areaUnderROC(I)D: I7 Branch 7 IFNONNULL L475 - false
   * Goal 4. weka.classifiers.Evaluation.weightedAreaUnderROC()D: I21 Branch 8 IF_ICMPGE L493 - true
   * Goal 5. weka.classifiers.Evaluation.weightedAreaUnderROC()D: I21 Branch 8 IF_ICMPGE L493 - false
   * Goal 6. weka.classifiers.Evaluation.weightedAreaUnderROC()D: I33 Branch 9 IF_ICMPGE L494 - true
   * Goal 7. weka.classifiers.Evaluation.weightedAreaUnderROC()D: I33 Branch 9 IF_ICMPGE L494 - false
   * Goal 8. weka.classifiers.Evaluation.weightedAreaUnderROC()D: I85 Branch 10 IF_ICMPGE L501 - true
   * Goal 9. weka.classifiers.Evaluation.weightedAreaUnderROC()D: I85 Branch 10 IF_ICMPGE L501 - false
   * Goal 10. weka.classifiers.Evaluation.weightedAreaUnderROC()D: I99 Branch 11 IFNE L503 - true
   * Goal 11. weka.classifiers.Evaluation.toClassDetailsString(Ljava/lang/String;)Ljava/lang/String;: I7 Branch 252 IFNE L2714 - true
   * Goal 12. weka.classifiers.Evaluation.toClassDetailsString(Ljava/lang/String;)Ljava/lang/String;: I57 Branch 253 IF_ICMPGE L2721 - true
   * Goal 13. weka.classifiers.Evaluation.toClassDetailsString(Ljava/lang/String;)Ljava/lang/String;: I57 Branch 253 IF_ICMPGE L2721 - false
   * Goal 14. weka.classifiers.Evaluation.toClassDetailsString(Ljava/lang/String;)Ljava/lang/String;: I143 Branch 254 IFEQ L2731 - true
   * Goal 15. weka.classifiers.Evaluation.toClassDetailsString(Ljava/lang/String;)Ljava/lang/String;: I182 Branch 255 IFEQ L2740 - false
   * Goal 16. weka.classifiers.Evaluation.toClassDetailsString(Ljava/lang/String;)Ljava/lang/String;: I219 Branch 256 IFEQ L2746 - false
   * Goal 17. weka.classifiers.Evaluation.numTruePositives(I)D: I15 Branch 257 IF_ICMPGE L2785 - true
   * Goal 18. weka.classifiers.Evaluation.numTruePositives(I)D: I15 Branch 257 IF_ICMPGE L2785 - false
   * Goal 19. weka.classifiers.Evaluation.numTruePositives(I)D: I23 Branch 258 IF_ICMPNE L2786 - true
   * Goal 20. weka.classifiers.Evaluation.numTruePositives(I)D: I23 Branch 258 IF_ICMPNE L2786 - false
   * Goal 21. weka.classifiers.Evaluation.truePositiveRate(I)D: I18 Branch 259 IF_ICMPGE L2810 - true
   * Goal 22. weka.classifiers.Evaluation.truePositiveRate(I)D: I18 Branch 259 IF_ICMPGE L2810 - false
   * Goal 23. weka.classifiers.Evaluation.truePositiveRate(I)D: I26 Branch 260 IF_ICMPNE L2811 - true
   * Goal 24. weka.classifiers.Evaluation.truePositiveRate(I)D: I26 Branch 260 IF_ICMPNE L2811 - false
   * Goal 25. weka.classifiers.Evaluation.truePositiveRate(I)D: I64 Branch 261 IFNE L2816 - false
   * Goal 26. weka.classifiers.Evaluation.weightedTruePositiveRate()D: I21 Branch 262 IF_ICMPGE L2831 - true
   * Goal 27. weka.classifiers.Evaluation.weightedTruePositiveRate()D: I21 Branch 262 IF_ICMPGE L2831 - false
   * Goal 28. weka.classifiers.Evaluation.weightedTruePositiveRate()D: I33 Branch 263 IF_ICMPGE L2832 - true
   * Goal 29. weka.classifiers.Evaluation.weightedTruePositiveRate()D: I33 Branch 263 IF_ICMPGE L2832 - false
   * Goal 30. weka.classifiers.Evaluation.weightedTruePositiveRate()D: I85 Branch 264 IF_ICMPGE L2839 - true
   * Goal 31. weka.classifiers.Evaluation.weightedTruePositiveRate()D: I85 Branch 264 IF_ICMPGE L2839 - false
   * Goal 32. weka.classifiers.Evaluation.numTrueNegatives(I)D: I15 Branch 265 IF_ICMPGE L2862 - true
   * Goal 33. weka.classifiers.Evaluation.numTrueNegatives(I)D: I15 Branch 265 IF_ICMPGE L2862 - false
   * Goal 34. weka.classifiers.Evaluation.numTrueNegatives(I)D: I23 Branch 266 IF_ICMPEQ L2863 - true
   * Goal 35. weka.classifiers.Evaluation.numTrueNegatives(I)D: I23 Branch 266 IF_ICMPEQ L2863 - false
   * Goal 36. weka.classifiers.Evaluation.numTrueNegatives(I)D: I35 Branch 267 IF_ICMPGE L2864 - true
   * Goal 37. weka.classifiers.Evaluation.numTrueNegatives(I)D: I35 Branch 267 IF_ICMPGE L2864 - false
   * Goal 38. weka.classifiers.Evaluation.numTrueNegatives(I)D: I43 Branch 268 IF_ICMPEQ L2865 - true
   * Goal 39. weka.classifiers.Evaluation.numTrueNegatives(I)D: I43 Branch 268 IF_ICMPEQ L2865 - false
   * Goal 40. weka.classifiers.Evaluation.numFalsePositives(I)D: I15 Branch 277 IF_ICMPGE L2947 - true
   * Goal 41. weka.classifiers.Evaluation.numFalsePositives(I)D: I15 Branch 277 IF_ICMPGE L2947 - false
   * Goal 42. weka.classifiers.Evaluation.numFalsePositives(I)D: I23 Branch 278 IF_ICMPEQ L2948 - true
   * Goal 43. weka.classifiers.Evaluation.numFalsePositives(I)D: I23 Branch 278 IF_ICMPEQ L2948 - false
   * Goal 44. weka.classifiers.Evaluation.numFalsePositives(I)D: I35 Branch 279 IF_ICMPGE L2949 - true
   * Goal 45. weka.classifiers.Evaluation.numFalsePositives(I)D: I35 Branch 279 IF_ICMPGE L2949 - false
   * Goal 46. weka.classifiers.Evaluation.numFalsePositives(I)D: I43 Branch 280 IF_ICMPNE L2950 - true
   * Goal 47. weka.classifiers.Evaluation.numFalsePositives(I)D: I43 Branch 280 IF_ICMPNE L2950 - false
   * Goal 48. weka.classifiers.Evaluation.numFalseNegatives(I)D: I15 Branch 289 IF_ICMPGE L3032 - true
   * Goal 49. weka.classifiers.Evaluation.numFalseNegatives(I)D: I15 Branch 289 IF_ICMPGE L3032 - false
   * Goal 50. weka.classifiers.Evaluation.numFalseNegatives(I)D: I23 Branch 290 IF_ICMPNE L3033 - true
   * Goal 51. weka.classifiers.Evaluation.numFalseNegatives(I)D: I23 Branch 290 IF_ICMPNE L3033 - false
   * Goal 52. weka.classifiers.Evaluation.numFalseNegatives(I)D: I35 Branch 291 IF_ICMPGE L3034 - true
   * Goal 53. weka.classifiers.Evaluation.numFalseNegatives(I)D: I35 Branch 291 IF_ICMPGE L3034 - false
   * Goal 54. weka.classifiers.Evaluation.numFalseNegatives(I)D: I43 Branch 292 IF_ICMPEQ L3035 - true
   * Goal 55. weka.classifiers.Evaluation.numFalseNegatives(I)D: I43 Branch 292 IF_ICMPEQ L3035 - false
   * Goal 56. weka.classifiers.Evaluation.matthewsCorrelationCoefficient(I)D: I65 Branch 301 IFNE L3120 - false
   * Goal 57. weka.classifiers.Evaluation.weightedMatthewsCorrelation()D: I21 Branch 302 IF_ICMPGE L3136 - true
   * Goal 58. weka.classifiers.Evaluation.weightedMatthewsCorrelation()D: I21 Branch 302 IF_ICMPGE L3136 - false
   * Goal 59. weka.classifiers.Evaluation.weightedMatthewsCorrelation()D: I33 Branch 303 IF_ICMPGE L3137 - true
   * Goal 60. weka.classifiers.Evaluation.weightedMatthewsCorrelation()D: I33 Branch 303 IF_ICMPGE L3137 - false
   * Goal 61. weka.classifiers.Evaluation.weightedMatthewsCorrelation()D: I85 Branch 304 IF_ICMPGE L3144 - true
   * Goal 62. weka.classifiers.Evaluation.weightedMatthewsCorrelation()D: I85 Branch 304 IF_ICMPGE L3144 - false
   * Goal 63. weka.classifiers.Evaluation.weightedMatthewsCorrelation()D: I99 Branch 305 IFNE L3146 - false
   * Goal 64. weka.classifiers.Evaluation.precision(I)D: I18 Branch 306 IF_ICMPGE L3200 - true
   * Goal 65. weka.classifiers.Evaluation.precision(I)D: I18 Branch 306 IF_ICMPGE L3200 - false
   * Goal 66. weka.classifiers.Evaluation.precision(I)D: I26 Branch 307 IF_ICMPNE L3201 - true
   * Goal 67. weka.classifiers.Evaluation.precision(I)D: I26 Branch 307 IF_ICMPNE L3201 - false
   * Goal 68. weka.classifiers.Evaluation.precision(I)D: I64 Branch 308 IFNE L3206 - false
   * Goal 69. weka.classifiers.Evaluation.weightedPrecision()D: I21 Branch 309 IF_ICMPGE L3221 - true
   * Goal 70. weka.classifiers.Evaluation.weightedPrecision()D: I21 Branch 309 IF_ICMPGE L3221 - false
   * Goal 71. weka.classifiers.Evaluation.weightedPrecision()D: I33 Branch 310 IF_ICMPGE L3222 - true
   * Goal 72. weka.classifiers.Evaluation.weightedPrecision()D: I33 Branch 310 IF_ICMPGE L3222 - false
   * Goal 73. weka.classifiers.Evaluation.weightedPrecision()D: I85 Branch 311 IF_ICMPGE L3229 - true
   * Goal 74. weka.classifiers.Evaluation.weightedPrecision()D: I85 Branch 311 IF_ICMPGE L3229 - false
   * Goal 75. weka.classifiers.Evaluation.fMeasure(I)D: I22 Branch 312 IFNE L3255 - false
   * Goal 76. weka.classifiers.Evaluation.weightedFMeasure()D: I21 Branch 313 IF_ICMPGE L3270 - true
   * Goal 77. weka.classifiers.Evaluation.weightedFMeasure()D: I21 Branch 313 IF_ICMPGE L3270 - false
   * Goal 78. weka.classifiers.Evaluation.weightedFMeasure()D: I33 Branch 314 IF_ICMPGE L3271 - true
   * Goal 79. weka.classifiers.Evaluation.weightedFMeasure()D: I33 Branch 314 IF_ICMPGE L3271 - false
   * Goal 80. weka.classifiers.Evaluation.weightedFMeasure()D: I85 Branch 315 IF_ICMPGE L3278 - true
   * Goal 81. weka.classifiers.Evaluation.weightedFMeasure()D: I85 Branch 315 IF_ICMPGE L3278 - false
   */

  @Test
  public void test04()  throws Throwable  {
      Bagging bagging0 = new Bagging();
      assertEquals("Whether the out-of-bag error is calculated.", bagging0.calcOutOfBagTipText());
      assertEquals(10, bagging0.getNumIterations());
      assertEquals(1, bagging0.getSeed());
      assertEquals(100, bagging0.getBagSizePercent());
      assertEquals("The number of iterations to be performed.", bagging0.numIterationsTipText());
      assertEquals("The base classifier to be used.", bagging0.classifierTipText());
      assertFalse(bagging0.getDebug());
      assertEquals("Size of each bag, as a percentage of the training set size.", bagging0.bagSizePercentTipText());
      assertEquals(0.0, bagging0.measureOutOfBagError(), 0.01D);
      assertEquals(1, bagging0.getNumExecutionSlots());
      assertEquals("If set to true, classifier may output additional info to the console.", bagging0.debugTipText());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", bagging0.numExecutionSlotsTipText());
      assertEquals("The random number seed to be used.", bagging0.seedTipText());
      assertFalse(bagging0.getCalcOutOfBag());
      assertNotNull(bagging0);
      
      Capabilities capabilities0 = bagging0.getCapabilities();
      assertEquals("Whether the out-of-bag error is calculated.", bagging0.calcOutOfBagTipText());
      assertEquals(10, bagging0.getNumIterations());
      assertEquals(1, bagging0.getSeed());
      assertEquals(100, bagging0.getBagSizePercent());
      assertEquals("The number of iterations to be performed.", bagging0.numIterationsTipText());
      assertEquals("The base classifier to be used.", bagging0.classifierTipText());
      assertFalse(bagging0.getDebug());
      assertEquals("Size of each bag, as a percentage of the training set size.", bagging0.bagSizePercentTipText());
      assertEquals(0.0, bagging0.measureOutOfBagError(), 0.01D);
      assertEquals(1, bagging0.getNumExecutionSlots());
      assertEquals("If set to true, classifier may output additional info to the console.", bagging0.debugTipText());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", bagging0.numExecutionSlotsTipText());
      assertEquals("The random number seed to be used.", bagging0.seedTipText());
      assertFalse(bagging0.getCalcOutOfBag());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertNotNull(capabilities0);
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals("Whether the out-of-bag error is calculated.", bagging0.calcOutOfBagTipText());
      assertEquals(10, bagging0.getNumIterations());
      assertEquals(1, bagging0.getSeed());
      assertEquals(100, bagging0.getBagSizePercent());
      assertEquals("The number of iterations to be performed.", bagging0.numIterationsTipText());
      assertEquals("The base classifier to be used.", bagging0.classifierTipText());
      assertFalse(bagging0.getDebug());
      assertEquals("Size of each bag, as a percentage of the training set size.", bagging0.bagSizePercentTipText());
      assertEquals(0.0, bagging0.measureOutOfBagError(), 0.01D);
      assertEquals(1, bagging0.getNumExecutionSlots());
      assertEquals("If set to true, classifier may output additional info to the console.", bagging0.debugTipText());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", bagging0.numExecutionSlotsTipText());
      assertEquals("The random number seed to be used.", bagging0.seedTipText());
      assertFalse(bagging0.getCalcOutOfBag());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate("Average Cost                      ");
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(4, instances0.numClasses());
      assertEquals("Whether the out-of-bag error is calculated.", bagging0.calcOutOfBagTipText());
      assertEquals(10, bagging0.getNumIterations());
      assertEquals(1, bagging0.getSeed());
      assertEquals(100, bagging0.getBagSizePercent());
      assertEquals("The number of iterations to be performed.", bagging0.numIterationsTipText());
      assertEquals("The base classifier to be used.", bagging0.classifierTipText());
      assertFalse(bagging0.getDebug());
      assertEquals("Size of each bag, as a percentage of the training set size.", bagging0.bagSizePercentTipText());
      assertEquals(0.0, bagging0.measureOutOfBagError(), 0.01D);
      assertEquals(1, bagging0.getNumExecutionSlots());
      assertEquals("If set to true, classifier may output additional info to the console.", bagging0.debugTipText());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", bagging0.numExecutionSlotsTipText());
      assertEquals("The random number seed to be used.", bagging0.seedTipText());
      assertFalse(bagging0.getCalcOutOfBag());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(4, instances0.numClasses());
      assertEquals("Whether the out-of-bag error is calculated.", bagging0.calcOutOfBagTipText());
      assertEquals(10, bagging0.getNumIterations());
      assertEquals(1, bagging0.getSeed());
      assertEquals(100, bagging0.getBagSizePercent());
      assertEquals("The number of iterations to be performed.", bagging0.numIterationsTipText());
      assertEquals("The base classifier to be used.", bagging0.classifierTipText());
      assertFalse(bagging0.getDebug());
      assertEquals("Size of each bag, as a percentage of the training set size.", bagging0.bagSizePercentTipText());
      assertEquals(0.0, bagging0.measureOutOfBagError(), 0.01D);
      assertEquals(1, bagging0.getNumExecutionSlots());
      assertEquals("If set to true, classifier may output additional info to the console.", bagging0.debugTipText());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", bagging0.numExecutionSlotsTipText());
      assertEquals("The random number seed to be used.", bagging0.seedTipText());
      assertFalse(bagging0.getCalcOutOfBag());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertNotNull(evaluation0);
      
      String string0 = evaluation0.toClassDetailsString("Average Cost                      ");
      assertEquals("Average Cost                      \n                 TP Rate  FP Rate  Precision  Recall  F-Measure  MCC    ROC Area  PRC Area  Class\n                 0        0        0          0       0          0     ?         ?         class1\n                 0        0        0          0       0          0     ?         ?         class2\n                 0        0        0          0       0          0     ?         ?         class3\n                 0        0        0          0       0          0     ?         ?         class4\nWeighted Avg.  NaN      NaN      NaN        NaN     NaN        NaN    NaN       NaN    \n", string0);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.size());
      assertEquals(3, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(4, instances0.numClasses());
      assertEquals("Whether the out-of-bag error is calculated.", bagging0.calcOutOfBagTipText());
      assertEquals(10, bagging0.getNumIterations());
      assertEquals(1, bagging0.getSeed());
      assertEquals(100, bagging0.getBagSizePercent());
      assertEquals("The number of iterations to be performed.", bagging0.numIterationsTipText());
      assertEquals("The base classifier to be used.", bagging0.classifierTipText());
      assertFalse(bagging0.getDebug());
      assertEquals("Size of each bag, as a percentage of the training set size.", bagging0.bagSizePercentTipText());
      assertEquals(0.0, bagging0.measureOutOfBagError(), 0.01D);
      assertEquals(1, bagging0.getNumExecutionSlots());
      assertEquals("If set to true, classifier may output additional info to the console.", bagging0.debugTipText());
      assertEquals("The number of execution slots (threads) to use for constructing the ensemble.", bagging0.numExecutionSlotsTipText());
      assertEquals("The random number seed to be used.", bagging0.seedTipText());
      assertFalse(bagging0.getCalcOutOfBag());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertNotNull(string0);
  }

  //Test case number: 5
  /*
   * 3 covered goals:
   * Goal 1. weka.classifiers.Evaluation.kappa()D: I30 Branch 193 IF_ICMPGE L2158 - true
   * Goal 2. weka.classifiers.Evaluation.kappa()D: I116 Branch 195 IF_ICMPGE L2166 - true
   * Goal 3. weka.classifiers.Evaluation.kappa()D: I169 Branch 196 IFGE L2173 - true
   */

  @Test
  public void test05()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertNotNull(textDirectoryLoader0);
      
      ConverterUtils.DataSource converterUtils_DataSource0 = new ConverterUtils.DataSource((Loader) textDirectoryLoader0);
      assertTrue(converterUtils_DataSource0.isIncremental());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertNotNull(converterUtils_DataSource0);
      
      Instances instances0 = converterUtils_DataSource0.getStructure();
      assertTrue(converterUtils_DataSource0.isIncremental());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(0, instances0.numInstances());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertTrue(converterUtils_DataSource0.isIncremental());
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(0, instances0.numInstances());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertNotNull(evaluation0);
      
      double double0 = evaluation0.kappa();
      assertEquals(1.0, double0, 0.01D);
      assertTrue(converterUtils_DataSource0.isIncremental());
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(0, instances0.numInstances());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
  }

  //Test case number: 6
  /*
   * 1 covered goal:
   * Goal 1. weka.classifiers.Evaluation.toClassDetailsString(Ljava/lang/String;)Ljava/lang/String;: I7 Branch 252 IFNE L2714 - false
   */

  @Test
  public void test06()  throws Throwable  {
      CoverTree coverTree0 = new CoverTree();
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", coverTree0.distanceFunctionTipText());
      assertEquals(0.0, coverTree0.measureNumLeaves(), 0.01D);
      assertEquals(0.0, coverTree0.measureTreeSize(), 0.01D);
      assertEquals(0.0, coverTree0.measureMaxDepth(), 0.01D);
      assertFalse(coverTree0.getMeasurePerformance());
      assertEquals("The base for the expansion constant.", coverTree0.baseTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", coverTree0.measurePerformanceTipText());
      assertEquals(1.3, coverTree0.getBase(), 0.01D);
      assertNotNull(coverTree0);
      
      AdditiveRegression additiveRegression0 = new AdditiveRegression();
      assertEquals(1.0, additiveRegression0.getShrinkage(), 0.01D);
      assertEquals(0.0, additiveRegression0.measureNumIterations(), 0.01D);
      assertEquals(10, additiveRegression0.getNumIterations());
      assertFalse(additiveRegression0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", additiveRegression0.debugTipText());
      assertEquals("The number of iterations to be performed.", additiveRegression0.numIterationsTipText());
      assertEquals("The base classifier to be used.", additiveRegression0.classifierTipText());
      assertEquals("Shrinkage rate. Smaller values help prevent overfitting and have a smoothing effect (but increase learning time). Default = 1.0, ie. no shrinkage.", additiveRegression0.shrinkageTipText());
      assertNotNull(additiveRegression0);
      
      String string0 = coverTree0.globalInfo();
      assertEquals("Class implementing the CoverTree datastructure.\nThe class is very much a translation of the c source code made available by the authors.\n\nFor more information and original source code see:\n\nAlina Beygelzimer, Sham Kakade, John Langford: Cover trees for nearest neighbor. In: ICML'06: Proceedings of the 23rd international conference on Machine learning, New York, NY, USA, 97-104, 2006.", string0);
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", coverTree0.distanceFunctionTipText());
      assertEquals(0.0, coverTree0.measureNumLeaves(), 0.01D);
      assertEquals(0.0, coverTree0.measureTreeSize(), 0.01D);
      assertEquals(0.0, coverTree0.measureMaxDepth(), 0.01D);
      assertFalse(coverTree0.getMeasurePerformance());
      assertEquals("The base for the expansion constant.", coverTree0.baseTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", coverTree0.measurePerformanceTipText());
      assertEquals(1.3, coverTree0.getBase(), 0.01D);
      assertNotNull(string0);
      
      Capabilities capabilities0 = additiveRegression0.getCapabilities();
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1.0, additiveRegression0.getShrinkage(), 0.01D);
      assertEquals(0.0, additiveRegression0.measureNumIterations(), 0.01D);
      assertEquals(10, additiveRegression0.getNumIterations());
      assertFalse(additiveRegression0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", additiveRegression0.debugTipText());
      assertEquals("The number of iterations to be performed.", additiveRegression0.numIterationsTipText());
      assertEquals("The base classifier to be used.", additiveRegression0.classifierTipText());
      assertEquals("Shrinkage rate. Smaller values help prevent overfitting and have a smoothing effect (but increase learning time). Default = 1.0, ie. no shrinkage.", additiveRegression0.shrinkageTipText());
      assertNotNull(capabilities0);
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1.0, additiveRegression0.getShrinkage(), 0.01D);
      assertEquals(0.0, additiveRegression0.measureNumIterations(), 0.01D);
      assertEquals(10, additiveRegression0.getNumIterations());
      assertFalse(additiveRegression0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", additiveRegression0.debugTipText());
      assertEquals("The number of iterations to be performed.", additiveRegression0.numIterationsTipText());
      assertEquals("The base classifier to be used.", additiveRegression0.classifierTipText());
      assertEquals("Shrinkage rate. Smaller values help prevent overfitting and have a smoothing effect (but increase learning time). Default = 1.0, ie. no shrinkage.", additiveRegression0.shrinkageTipText());
      assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate("Class implementing the CoverTree datastructure.\nThe class is very much a translation of the c source code made available by the authors.\n\nFor more information and original source code see:\n\nAlina Beygelzimer, Sham Kakade, John Langford: Cover trees for nearest neighbor. In: ICML'06: Proceedings of the 23rd international conference on Machine learning, New York, NY, USA, 97-104, 2006.");
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(3, instances0.classIndex());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(4, instances0.numAttributes());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1.0, additiveRegression0.getShrinkage(), 0.01D);
      assertEquals(0.0, additiveRegression0.measureNumIterations(), 0.01D);
      assertEquals(10, additiveRegression0.getNumIterations());
      assertFalse(additiveRegression0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", additiveRegression0.debugTipText());
      assertEquals("The number of iterations to be performed.", additiveRegression0.numIterationsTipText());
      assertEquals("The base classifier to be used.", additiveRegression0.classifierTipText());
      assertEquals("Shrinkage rate. Smaller values help prevent overfitting and have a smoothing effect (but increase learning time). Default = 1.0, ie. no shrinkage.", additiveRegression0.shrinkageTipText());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(3, instances0.classIndex());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(4, instances0.numAttributes());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1.0, additiveRegression0.getShrinkage(), 0.01D);
      assertEquals(0.0, additiveRegression0.measureNumIterations(), 0.01D);
      assertEquals(10, additiveRegression0.getNumIterations());
      assertFalse(additiveRegression0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", additiveRegression0.debugTipText());
      assertEquals("The number of iterations to be performed.", additiveRegression0.numIterationsTipText());
      assertEquals("The base classifier to be used.", additiveRegression0.classifierTipText());
      assertEquals("Shrinkage rate. Smaller values help prevent overfitting and have a smoothing effect (but increase learning time). Default = 1.0, ie. no shrinkage.", additiveRegression0.shrinkageTipText());
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertNotNull(evaluation0);
      
      try {
        String string1 = evaluation0.toClassDetailsString("Class implementing the CoverTree datastructure.\nThe class is very much a translation of the c source code made available by the authors.\n\nFor more information and original source code see:\n\nAlina Beygelzimer, Sham Kakade, John Langford: Cover trees for nearest neighbor. In: ICML'06: Proceedings of the 23rd international conference on Machine learning, New York, NY, USA, 97-104, 2006.");
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // Evaluation: No per class statistics possible!
         //
      }
  }

  //Test case number: 7
  /*
   * 2 covered goals:
   * Goal 1. weka.classifiers.Evaluation.evaluateModel(Ljava/lang/String;[Ljava/lang/String;)Ljava/lang/String;: root-Branch
   * Goal 2. weka.classifiers.Evaluation.main([Ljava/lang/String;)V: I7 Branch 22 IFNE L801 - true
   */

  @Test
  public void test07()  throws Throwable  {
      String[] stringArray0 = new String[1];
      Evaluation.main(stringArray0);
  }

  //Test case number: 8
  @Test
  public void test08()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertNotNull(textDirectoryLoader0);
      
      Instances instances0 = textDirectoryLoader0.getStructure();
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(0, instances0.size());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(0, instances0.size());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertNotNull(evaluation0);
      
      char[] charArray0 = new char[18];
      charArray0[14] = '\u008C';
      charArray0[5] = '\u008C';
      String string0 = evaluation0.num2ShortID('\u008C', charArray0, '\u008C');
      assertEquals("                                                                                                                                          \u0000\u008C", string0);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(0, instances0.size());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertNotNull(string0);
  }

  //Test case number: 9
  /*
   * 2 covered goals:
   * Goal 1. weka.classifiers.Evaluation.makeDistribution(D)[D: I13 Branch 360 IFEQ L3675 - true
   * Goal 2. weka.classifiers.Evaluation.makeDistribution(D)[D: I25 Branch 361 IFEQ L3678 - false
   */

  @Test
  public void test09()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getDebug());
      assertNotNull(textDirectoryLoader0);
      
      Instances instances0 = textDirectoryLoader0.getStructure();
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.size());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(0, instances0.numClasses());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getDebug());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.size());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(0, instances0.numClasses());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertNotNull(evaluation0);
      
      // Undeclared exception!
      try {
        double[] doubleArray0 = evaluation0.makeDistribution(427.9);
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // 427
         //
      }
  }

  //Test case number: 10
  /*
   * 15 covered goals:
   * Goal 1. weka.classifiers.Evaluation.falsePositiveRate(I)D: I18 Branch 281 IF_ICMPGE L2976 - true
   * Goal 2. weka.classifiers.Evaluation.falsePositiveRate(I)D: I18 Branch 281 IF_ICMPGE L2976 - false
   * Goal 3. weka.classifiers.Evaluation.falsePositiveRate(I)D: I26 Branch 282 IF_ICMPEQ L2977 - true
   * Goal 4. weka.classifiers.Evaluation.falsePositiveRate(I)D: I26 Branch 282 IF_ICMPEQ L2977 - false
   * Goal 5. weka.classifiers.Evaluation.falsePositiveRate(I)D: I38 Branch 283 IF_ICMPGE L2978 - true
   * Goal 6. weka.classifiers.Evaluation.falsePositiveRate(I)D: I38 Branch 283 IF_ICMPGE L2978 - false
   * Goal 7. weka.classifiers.Evaluation.falsePositiveRate(I)D: I46 Branch 284 IF_ICMPNE L2979 - true
   * Goal 8. weka.classifiers.Evaluation.falsePositiveRate(I)D: I46 Branch 284 IF_ICMPNE L2979 - false
   * Goal 9. weka.classifiers.Evaluation.falsePositiveRate(I)D: I91 Branch 285 IFNE L2986 - false
   * Goal 10. weka.classifiers.Evaluation.weightedFalsePositiveRate()D: I21 Branch 286 IF_ICMPGE L3001 - true
   * Goal 11. weka.classifiers.Evaluation.weightedFalsePositiveRate()D: I21 Branch 286 IF_ICMPGE L3001 - false
   * Goal 12. weka.classifiers.Evaluation.weightedFalsePositiveRate()D: I33 Branch 287 IF_ICMPGE L3002 - true
   * Goal 13. weka.classifiers.Evaluation.weightedFalsePositiveRate()D: I33 Branch 287 IF_ICMPGE L3002 - false
   * Goal 14. weka.classifiers.Evaluation.weightedFalsePositiveRate()D: I85 Branch 288 IF_ICMPGE L3009 - true
   * Goal 15. weka.classifiers.Evaluation.weightedFalsePositiveRate()D: I85 Branch 288 IF_ICMPGE L3009 - false
   */

  @Test
  public void test10()  throws Throwable  {
      SGD sGD0 = new SGD();
      assertEquals(0, sGD0.HINGE);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(1, sGD0.getSeed());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertFalse(sGD0.getDontNormalize());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals(500, sGD0.getEpochs());
      assertFalse(sGD0.getDebug());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertNotNull(sGD0);
      
      Capabilities capabilities0 = sGD0.getCapabilities();
      assertEquals(0, sGD0.HINGE);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(1, sGD0.getSeed());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertFalse(sGD0.getDontNormalize());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals(500, sGD0.getEpochs());
      assertFalse(sGD0.getDebug());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertNotNull(capabilities0);
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals(0, sGD0.HINGE);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(3, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, sGD0.getSeed());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertFalse(sGD0.getDontNormalize());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals(500, sGD0.getEpochs());
      assertFalse(sGD0.getDebug());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate("wOn{|th@`)");
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals(0, sGD0.HINGE);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(3, instances0.numAttributes());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.classIndex());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(3, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, sGD0.getSeed());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertFalse(sGD0.getDontNormalize());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals(500, sGD0.getEpochs());
      assertFalse(sGD0.getDebug());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals(0, sGD0.HINGE);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(3, instances0.numAttributes());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.classIndex());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(3, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(1, sGD0.getSeed());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertFalse(sGD0.getDontNormalize());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals(500, sGD0.getEpochs());
      assertFalse(sGD0.getDebug());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertNotNull(evaluation0);
      
      double double0 = evaluation0.weightedFalsePositiveRate();
      assertEquals(Double.NaN, double0, 0.01D);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals(0, sGD0.HINGE);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(3, instances0.numAttributes());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.classIndex());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(3, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(1, sGD0.getSeed());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertFalse(sGD0.getDontNormalize());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals(500, sGD0.getEpochs());
      assertFalse(sGD0.getDebug());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
  }

  //Test case number: 11
  /*
   * 1 covered goal:
   * Goal 1. weka.classifiers.Evaluation.evaluateModelOnceAndRecordPrediction(Lweka/classifiers/Classifier;Lweka/core/Instance;)D: root-Branch
   */

  @Test
  public void test11()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertNotNull(textDirectoryLoader0);
      
      Instances instances0 = textDirectoryLoader0.getStructure();
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(0, instances0.size());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(0, instances0.size());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertNotNull(evaluation0);
      
      DatabaseLoader databaseLoader0 = new DatabaseLoader();
      assertEquals("For incremental loading a unique identiefer has to be specified.\nIf the query includes all columns of a table (SELECT *...) a primary key\ncan be detected automatically depending on the JDBC driver. If that is not possible\nspecify the key columns here in a comma separated list.", databaseLoader0.keysTipText());
      assertEquals("", databaseLoader0.getPassword());
      assertEquals("The database password", databaseLoader0.passwordTipText());
      assertEquals("The user name for the database", databaseLoader0.userTipText());
      assertEquals("Reads Instances from a Database. Can read a database in batch or incremental mode.\nIn inremental mode MySQL and HSQLDB are supported.\nFor all other DBMS set a pseudoincremental mode is used:\nIn pseudo incremental mode the instances are read into main memory all at once and then incrementally provided to the user.\nFor incremental loading the rows in the database table have to be ordered uniquely.\nThe reason for this is that every time only a single row is fetched by extending the user query by a LIMIT clause.\nIf this extension is impossible instances will be loaded pseudoincrementally. To ensure that every row is fetched exaclty once, they have to ordered.\nTherefore a (primary) key is necessary.This approach is chosen, instead of using JDBC driver facilities, because the latter one differ betweeen different drivers.\nIf you use the DatabaseSaver and save instances by generating automatically a primary key (its name is defined in DtabaseUtils), this primary key will be used for ordering but will not be part of the output. The user defined SQL query to extract the instances should not contain LIMIT and ORDER BY clauses (see -Q option).\nIn addition, for incremental loading,  you can define in the DatabaseUtils file how many distinct values a nominal attribute is allowed to have. If this number is exceeded, the column will become a string attribute.\nIn batch mode no string attributes will be created.", databaseLoader0.globalInfo());
      assertEquals("", databaseLoader0.getUser());
      assertEquals("The URL of the database", databaseLoader0.urlTipText());
      assertEquals("The query that should load the instances.\n The query has to be of the form SELECT <column-list>|* FROM <table> [WHERE <conditions>]", databaseLoader0.queryTipText());
      assertEquals("Select * from Results0", databaseLoader0.getQuery());
      assertEquals("The custom properties that the user can use to override the default ones.", databaseLoader0.customPropsFileTipText());
      assertFalse(databaseLoader0.getSparseData());
      assertEquals("Encode data as sparse instances.", databaseLoader0.sparseDataTipText());
      assertEquals("jdbc:idb=experiments.prp", databaseLoader0.getUrl());
      assertNotNull(databaseLoader0);
      
      Instance instance0 = databaseLoader0.getNextInstance(instances0);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals(0, instances0.size());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertEquals("For incremental loading a unique identiefer has to be specified.\nIf the query includes all columns of a table (SELECT *...) a primary key\ncan be detected automatically depending on the JDBC driver. If that is not possible\nspecify the key columns here in a comma separated list.", databaseLoader0.keysTipText());
      assertEquals("", databaseLoader0.getPassword());
      assertEquals("The database password", databaseLoader0.passwordTipText());
      assertEquals("The user name for the database", databaseLoader0.userTipText());
      assertEquals("Reads Instances from a Database. Can read a database in batch or incremental mode.\nIn inremental mode MySQL and HSQLDB are supported.\nFor all other DBMS set a pseudoincremental mode is used:\nIn pseudo incremental mode the instances are read into main memory all at once and then incrementally provided to the user.\nFor incremental loading the rows in the database table have to be ordered uniquely.\nThe reason for this is that every time only a single row is fetched by extending the user query by a LIMIT clause.\nIf this extension is impossible instances will be loaded pseudoincrementally. To ensure that every row is fetched exaclty once, they have to ordered.\nTherefore a (primary) key is necessary.This approach is chosen, instead of using JDBC driver facilities, because the latter one differ betweeen different drivers.\nIf you use the DatabaseSaver and save instances by generating automatically a primary key (its name is defined in DtabaseUtils), this primary key will be used for ordering but will not be part of the output. The user defined SQL query to extract the instances should not contain LIMIT and ORDER BY clauses (see -Q option).\nIn addition, for incremental loading,  you can define in the DatabaseUtils file how many distinct values a nominal attribute is allowed to have. If this number is exceeded, the column will become a string attribute.\nIn batch mode no string attributes will be created.", databaseLoader0.globalInfo());
      assertEquals("", databaseLoader0.getUser());
      assertEquals("The URL of the database", databaseLoader0.urlTipText());
      assertEquals("The query that should load the instances.\n The query has to be of the form SELECT <column-list>|* FROM <table> [WHERE <conditions>]", databaseLoader0.queryTipText());
      assertEquals("Select * from Results0", databaseLoader0.getQuery());
      assertEquals("The custom properties that the user can use to override the default ones.", databaseLoader0.customPropsFileTipText());
      assertFalse(databaseLoader0.getSparseData());
      assertEquals("Encode data as sparse instances.", databaseLoader0.sparseDataTipText());
      assertEquals("jdbc:idb=experiments.prp", databaseLoader0.getUrl());
      assertNull(instance0);
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, sGDText0.HINGE);
      assertEquals(1, sGDText0.LOGLOSS);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1, sGDText0.getSeed());
      assertEquals(1.0, sGDText0.getNorm(), 0.01D);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01D);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01D);
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01D);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01D);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertNotNull(sGDText0);
      
      try {
        double double0 = evaluation0.evaluateModelOnceAndRecordPrediction((Classifier) sGDText0, (Instance) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  //Test case number: 12
  /*
   * 4 covered goals:
   * Goal 1. weka.classifiers.Evaluation.makeOptionString(Lweka/classifiers/Classifier;Z)Ljava/lang/String;: I360 Branch 350 IFEQ L3562 - false
   * Goal 2. weka.classifiers.Evaluation.getGlobalInfo(Lweka/classifiers/Classifier;)Ljava/lang/String;: I42 Branch 355 IF_ICMPGE L3625 - false
   * Goal 3. weka.classifiers.Evaluation.getGlobalInfo(Lweka/classifiers/Classifier;)Ljava/lang/String;: I65 Branch 356 IFLE L3628 - true
   * Goal 4. weka.classifiers.Evaluation.getGlobalInfo(Lweka/classifiers/Classifier;)Ljava/lang/String;: I65 Branch 356 IFLE L3628 - false
   */

  @Test
  public void test12()  throws Throwable  {
      OneR oneR0 = new OneR();
      assertEquals("If set to true, classifier may output additional info to the console.", oneR0.debugTipText());
      assertEquals("The minimum bucket size used for discretizing numeric attributes.", oneR0.minBucketSizeTipText());
      assertFalse(oneR0.getDebug());
      assertEquals(6, oneR0.getMinBucketSize());
      assertNotNull(oneR0);
      
      String string0 = Evaluation.makeOptionString(oneR0, true);
      assertEquals("\n\nGeneral options:\n\n-h or -help\n\tOutput help information.\n-synopsis or -info\n\tOutput synopsis for classifier (use in conjunction  with -h)\n-t <name of training file>\n\tSets training file.\n-T <name of test file>\n\tSets test file. If missing, a cross-validation will be performed\n\ton the training data.\n-c <class index>\n\tSets index of class attribute (default: last).\n-x <number of folds>\n\tSets number of folds for cross-validation (default: 10).\n-no-cv\n\tDo not perform any cross validation.\n-split-percentage <percentage>\n\tSets the percentage for the train/test set split, e.g., 66.\n-preserve-order\n\tPreserves the order in the percentage split.\n-s <random number seed>\n\tSets random number seed for cross-validation or percentage split\n\t(default: 1).\n-m <name of file with cost matrix>\n\tSets file with cost matrix.\n-l <name of input file>\n\tSets model input file. In case the filename ends with '.xml',\n\ta PMML file is loaded or, if that fails, options are loaded\n\tfrom the XML file.\n-d <name of output file>\n\tSets model output file. In case the filename ends with '.xml',\n\tonly the options are saved to the XML file, not the model.\n-v\n\tOutputs no statistics for training data.\n-o\n\tOutputs statistics only, not the classifier.\n-i\n\tOutputs detailed information-retrieval statistics for each class.\n-k\n\tOutputs information-theoretic statistics.\n-classifications \"weka.classifiers.evaluation.output.prediction.AbstractOutput + options\"\n\tUses the specified class for generating the classification output.\n\tE.g.: weka.classifiers.evaluation.output.prediction.PlainText\n-p range\n\tOutputs predictions for test instances (or the train instances if\n\tno test instances provided and -no-cv is used), along with the \n\tattributes in the specified range (and nothing else). \n\tUse '-p 0' if no attributes are desired.\n\tDeprecated: use \"-classifications ...\" instead.\n-distribution\n\tOutputs the distribution instead of only the prediction\n\tin conjunction with the '-p' option (only nominal classes).\n\tDeprecated: use \"-classifications ...\" instead.\n-r\n\tOnly outputs cumulative margin distribution.\n-z <class name>\n\tOnly outputs the source representation of the classifier,\n\tgiving it the supplied name.\n-xml filename | xml-string\n\tRetrieves the options from the XML-data instead of the command line.\n-threshold-file <file>\n\tThe file to save the threshold data to.\n\tThe format is determined by the extensions, e.g., '.arff' for ARFF \n\tformat or '.csv' for CSV.\n-threshold-label <label>\n\tThe class label to determine the threshold data for\n\t(default is the first label)\n\nOptions specific to weka.classifiers.rules.OneR:\n\n-B <minimum bucket size>\n\tThe minimum number of objects in a bucket (default: 6).\n\nSynopsis for weka.classifiers.rules.OneR:\n\nClass for building and using a 1R classifier; in other words, uses the minimum-error attribute for prediction, discretizing numeric attributes. For more information, see:\n\nR.C. Holte (1993). Very simple classification rules perform well on most commonly used datasets. Machine Learning. 11:63-91.", string0);
      assertEquals("If set to true, classifier may output additional info to the console.", oneR0.debugTipText());
      assertEquals("The minimum bucket size used for discretizing numeric attributes.", oneR0.minBucketSizeTipText());
      assertFalse(oneR0.getDebug());
      assertEquals(6, oneR0.getMinBucketSize());
      assertNotNull(string0);
  }

  //Test case number: 13
  /*
   * 1 covered goal:
   * Goal 1. weka.classifiers.Evaluation.correct()D: root-Branch
   */

  @Test
  public void test13()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertNotNull(textDirectoryLoader0);
      
      Instances instances0 = textDirectoryLoader0.getStructure();
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertNotNull(evaluation0);
      
      double double0 = evaluation0.correct();
      assertEquals(0.0, double0, 0.01D);
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
  }

  //Test case number: 14
  /*
   * 17 covered goals:
   * Goal 1. weka.classifiers.Evaluation.unclassified()D: root-Branch
   * Goal 2. weka.classifiers.Evaluation.meanAbsoluteError()D: root-Branch
   * Goal 3. weka.classifiers.Evaluation.toSummaryString()Ljava/lang/String;: root-Branch
   * Goal 4. weka.classifiers.Evaluation.areaUnderPRC(I)D: I7 Branch 12 IFNONNULL L521 - false
   * Goal 5. weka.classifiers.Evaluation.weightedAreaUnderPRC()D: I21 Branch 13 IF_ICMPGE L539 - true
   * Goal 6. weka.classifiers.Evaluation.weightedAreaUnderPRC()D: I21 Branch 13 IF_ICMPGE L539 - false
   * Goal 7. weka.classifiers.Evaluation.weightedAreaUnderPRC()D: I33 Branch 14 IF_ICMPGE L540 - true
   * Goal 8. weka.classifiers.Evaluation.weightedAreaUnderPRC()D: I33 Branch 14 IF_ICMPGE L540 - false
   * Goal 9. weka.classifiers.Evaluation.weightedAreaUnderPRC()D: I85 Branch 15 IF_ICMPGE L547 - true
   * Goal 10. weka.classifiers.Evaluation.weightedAreaUnderPRC()D: I85 Branch 15 IF_ICMPGE L547 - false
   * Goal 11. weka.classifiers.Evaluation.weightedAreaUnderPRC()D: I99 Branch 16 IFNE L549 - true
   * Goal 12. weka.classifiers.Evaluation.meanPriorAbsoluteError()D: I7 Branch 199 IFEQ L2229 - true
   * Goal 13. weka.classifiers.Evaluation.relativeAbsoluteError()D: I7 Branch 200 IFEQ L2243 - true
   * Goal 14. weka.classifiers.Evaluation.toSummaryString(Ljava/lang/String;Z)Ljava/lang/String;: I12 Branch 226 IFEQ L2512 - true
   * Goal 15. weka.classifiers.Evaluation.toSummaryString(Ljava/lang/String;Z)Ljava/lang/String;: I50 Branch 228 IFLE L2520 - true
   * Goal 16. weka.classifiers.Evaluation.toSummaryString(Ljava/lang/String;Z)Ljava/lang/String;: I625 Branch 237 IFEQ L2592 - true
   * Goal 17. weka.classifiers.Evaluation.toSummaryString(Ljava/lang/String;Z)Ljava/lang/String;: I689 Branch 238 IFLE L2599 - true
   */

  @Test
  public void test14()  throws Throwable  {
      SGD sGD0 = new SGD();
      //  // Unstable assertion: assertEquals(2, sGD0.SQUAREDLOSS);
      //  // Unstable assertion: assertEquals(0, sGD0.HINGE);
      //  // Unstable assertion: assertEquals(1, sGD0.LOGLOSS);
      //  // Unstable assertion: assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      //  // Unstable assertion: assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      //  // Unstable assertion: assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      //  // Unstable assertion: assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      //  // Unstable assertion: assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      //  // Unstable assertion: assertEquals(1, sGD0.getSeed());
      //  // Unstable assertion: assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      //  // Unstable assertion: assertFalse(sGD0.getDontReplaceMissing());
      //  // Unstable assertion: assertEquals("The random number seed to be used.", sGD0.seedTipText());
      //  // Unstable assertion: assertFalse(sGD0.getDontNormalize());
      //  // Unstable assertion: assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      //  // Unstable assertion: assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      //  // Unstable assertion: assertEquals(500, sGD0.getEpochs());
      //  // Unstable assertion: assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      //  // Unstable assertion: assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      //  // Unstable assertion: assertFalse(sGD0.getDebug());
      //  // Unstable assertion: assertNotNull(sGD0);
      
      Capabilities capabilities0 = sGD0.getCapabilities();
      //  // Unstable assertion: assertEquals(2, sGD0.SQUAREDLOSS);
      //  // Unstable assertion: assertEquals(0, sGD0.HINGE);
      //  // Unstable assertion: assertEquals(1, sGD0.LOGLOSS);
      //  // Unstable assertion: assertEquals(0, capabilities0.getMinimumNumberInstances());
      //  // Unstable assertion: assertFalse(capabilities0.hasDependencies());
      //  // Unstable assertion: assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      //  // Unstable assertion: assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      //  // Unstable assertion: assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      //  // Unstable assertion: assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      //  // Unstable assertion: assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      //  // Unstable assertion: assertEquals(1, sGD0.getSeed());
      //  // Unstable assertion: assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      //  // Unstable assertion: assertFalse(sGD0.getDontReplaceMissing());
      //  // Unstable assertion: assertEquals("The random number seed to be used.", sGD0.seedTipText());
      //  // Unstable assertion: assertFalse(sGD0.getDontNormalize());
      //  // Unstable assertion: assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      //  // Unstable assertion: assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      //  // Unstable assertion: assertEquals(500, sGD0.getEpochs());
      //  // Unstable assertion: assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      //  // Unstable assertion: assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      //  // Unstable assertion: assertFalse(sGD0.getDebug());
      //  // Unstable assertion: assertNotNull(capabilities0);
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      //  // Unstable assertion: assertEquals((-2), testInstances0.NO_CLASS);
      //  // Unstable assertion: assertEquals((-1), testInstances0.CLASS_IS_LAST);
      //  // Unstable assertion: assertEquals(2, sGD0.SQUAREDLOSS);
      //  // Unstable assertion: assertEquals(0, sGD0.HINGE);
      //  // Unstable assertion: assertEquals(1, sGD0.LOGLOSS);
      //  // Unstable assertion: assertEquals(3, testInstances0.getNumAttributes());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumRelational());
      //  // Unstable assertion: assertEquals(2, testInstances0.getNumClasses());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumNumeric());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumString());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumRelationalDate());
      //  // Unstable assertion: assertEquals(1, testInstances0.getClassType());
      //  // Unstable assertion: assertEquals(-1, testInstances0.getClassIndex());
      //  // Unstable assertion: assertFalse(testInstances0.getNoClass());
      //  // Unstable assertion: assertEquals(2, testInstances0.getNumNominalValues());
      //  // Unstable assertion: assertEquals(10, testInstances0.getNumInstancesRelational());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumDate());
      //  // Unstable assertion: assertEquals(2, testInstances0.getNumRelationalNominalValues());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumRelationalNumeric());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumRelationalString());
      //  // Unstable assertion: assertEquals("Testdata", testInstances0.getRelation());
      //  // Unstable assertion: assertEquals(20, testInstances0.getNumInstances());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumNominal());
      //  // Unstable assertion: assertEquals(" ", testInstances0.getWordSeparators());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumRelationalNominal());
      //  // Unstable assertion: assertFalse(testInstances0.getMultiInstance());
      //  // Unstable assertion: assertEquals(1, testInstances0.getSeed());
      //  // Unstable assertion: assertEquals(0, capabilities0.getMinimumNumberInstances());
      //  // Unstable assertion: assertFalse(capabilities0.hasDependencies());
      //  // Unstable assertion: assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      //  // Unstable assertion: assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      //  // Unstable assertion: assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      //  // Unstable assertion: assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      //  // Unstable assertion: assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      //  // Unstable assertion: assertEquals(1, sGD0.getSeed());
      //  // Unstable assertion: assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      //  // Unstable assertion: assertFalse(sGD0.getDontReplaceMissing());
      //  // Unstable assertion: assertEquals("The random number seed to be used.", sGD0.seedTipText());
      //  // Unstable assertion: assertFalse(sGD0.getDontNormalize());
      //  // Unstable assertion: assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      //  // Unstable assertion: assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      //  // Unstable assertion: assertEquals(500, sGD0.getEpochs());
      //  // Unstable assertion: assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      //  // Unstable assertion: assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      //  // Unstable assertion: assertFalse(sGD0.getDebug());
      //  // Unstable assertion: assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate("wOn{|th@`)");
      //  // Unstable assertion: assertEquals((-2), testInstances0.NO_CLASS);
      //  // Unstable assertion: assertEquals((-1), testInstances0.CLASS_IS_LAST);
      //  // Unstable assertion: assertEquals(2, sGD0.SQUAREDLOSS);
      //  // Unstable assertion: assertEquals(0, sGD0.HINGE);
      //  // Unstable assertion: assertEquals(1, sGD0.LOGLOSS);
      //  // Unstable assertion: assertEquals(3, testInstances0.getNumAttributes());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumRelational());
      //  // Unstable assertion: assertEquals(2, testInstances0.getNumClasses());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumNumeric());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumString());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumRelationalDate());
      //  // Unstable assertion: assertEquals(1, testInstances0.getClassType());
      //  // Unstable assertion: assertEquals(-1, testInstances0.getClassIndex());
      //  // Unstable assertion: assertFalse(testInstances0.getNoClass());
      //  // Unstable assertion: assertEquals(2, testInstances0.getNumNominalValues());
      //  // Unstable assertion: assertEquals(10, testInstances0.getNumInstancesRelational());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumDate());
      //  // Unstable assertion: assertEquals(2, testInstances0.getNumRelationalNominalValues());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumRelationalNumeric());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumRelationalString());
      //  // Unstable assertion: assertEquals("Testdata", testInstances0.getRelation());
      //  // Unstable assertion: assertEquals(20, testInstances0.getNumInstances());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumNominal());
      //  // Unstable assertion: assertEquals(" ", testInstances0.getWordSeparators());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumRelationalNominal());
      //  // Unstable assertion: assertFalse(testInstances0.getMultiInstance());
      //  // Unstable assertion: assertEquals(1, testInstances0.getSeed());
      //  // Unstable assertion: assertEquals(0, capabilities0.getMinimumNumberInstances());
      //  // Unstable assertion: assertFalse(capabilities0.hasDependencies());
      //  // Unstable assertion: assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      //  // Unstable assertion: assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      //  // Unstable assertion: assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      //  // Unstable assertion: assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      //  // Unstable assertion: assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      //  // Unstable assertion: assertEquals(1, sGD0.getSeed());
      //  // Unstable assertion: assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      //  // Unstable assertion: assertFalse(sGD0.getDontReplaceMissing());
      //  // Unstable assertion: assertEquals("The random number seed to be used.", sGD0.seedTipText());
      //  // Unstable assertion: assertFalse(sGD0.getDontNormalize());
      //  // Unstable assertion: assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      //  // Unstable assertion: assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      //  // Unstable assertion: assertEquals(500, sGD0.getEpochs());
      //  // Unstable assertion: assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      //  // Unstable assertion: assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      //  // Unstable assertion: assertFalse(sGD0.getDebug());
      //  // Unstable assertion: assertFalse(instances0.checkForStringAttributes());
      //  // Unstable assertion: assertEquals(2, instances0.numClasses());
      //  // Unstable assertion: assertEquals(20, instances0.size());
      //  // Unstable assertion: assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      //  // Unstable assertion: assertEquals("Testdata", instances0.relationName());
      //  // Unstable assertion: assertEquals(3, instances0.numAttributes());
      //  // Unstable assertion: assertEquals(20, instances0.numInstances());
      //  // Unstable assertion: assertEquals(2, instances0.classIndex());
      //  // Unstable assertion: assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      //  // Unstable assertion: assertEquals((-2), testInstances0.NO_CLASS);
      //  // Unstable assertion: assertEquals((-1), testInstances0.CLASS_IS_LAST);
      //  // Unstable assertion: assertEquals(2, sGD0.SQUAREDLOSS);
      //  // Unstable assertion: assertEquals(0, sGD0.HINGE);
      //  // Unstable assertion: assertEquals(1, sGD0.LOGLOSS);
      //  // Unstable assertion: assertEquals(3, testInstances0.getNumAttributes());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumRelational());
      //  // Unstable assertion: assertEquals(2, testInstances0.getNumClasses());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumNumeric());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumString());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumRelationalDate());
      //  // Unstable assertion: assertEquals(1, testInstances0.getClassType());
      //  // Unstable assertion: assertEquals(-1, testInstances0.getClassIndex());
      //  // Unstable assertion: assertFalse(testInstances0.getNoClass());
      //  // Unstable assertion: assertEquals(2, testInstances0.getNumNominalValues());
      //  // Unstable assertion: assertEquals(10, testInstances0.getNumInstancesRelational());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumDate());
      //  // Unstable assertion: assertEquals(2, testInstances0.getNumRelationalNominalValues());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumRelationalNumeric());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumRelationalString());
      //  // Unstable assertion: assertEquals("Testdata", testInstances0.getRelation());
      //  // Unstable assertion: assertEquals(20, testInstances0.getNumInstances());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumNominal());
      //  // Unstable assertion: assertEquals(" ", testInstances0.getWordSeparators());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumRelationalNominal());
      //  // Unstable assertion: assertFalse(testInstances0.getMultiInstance());
      //  // Unstable assertion: assertEquals(1, testInstances0.getSeed());
      //  // Unstable assertion: assertEquals(0, capabilities0.getMinimumNumberInstances());
      //  // Unstable assertion: assertFalse(capabilities0.hasDependencies());
      //  // Unstable assertion: assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      //  // Unstable assertion: assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      //  // Unstable assertion: assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      //  // Unstable assertion: assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      //  // Unstable assertion: assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      //  // Unstable assertion: assertEquals(1, sGD0.getSeed());
      //  // Unstable assertion: assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      //  // Unstable assertion: assertFalse(sGD0.getDontReplaceMissing());
      //  // Unstable assertion: assertEquals("The random number seed to be used.", sGD0.seedTipText());
      //  // Unstable assertion: assertFalse(sGD0.getDontNormalize());
      //  // Unstable assertion: assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      //  // Unstable assertion: assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      //  // Unstable assertion: assertEquals(500, sGD0.getEpochs());
      //  // Unstable assertion: assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      //  // Unstable assertion: assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      //  // Unstable assertion: assertFalse(sGD0.getDebug());
      //  // Unstable assertion: assertEquals(1.0, evaluation0.kappa(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      //  // Unstable assertion: assertFalse(evaluation0.getDiscardPredictions());
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.correct(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      //  // Unstable assertion: assertFalse(instances0.checkForStringAttributes());
      //  // Unstable assertion: assertEquals(2, instances0.numClasses());
      //  // Unstable assertion: assertEquals(20, instances0.size());
      //  // Unstable assertion: assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      //  // Unstable assertion: assertEquals("Testdata", instances0.relationName());
      //  // Unstable assertion: assertEquals(3, instances0.numAttributes());
      //  // Unstable assertion: assertEquals(20, instances0.numInstances());
      //  // Unstable assertion: assertEquals(2, instances0.classIndex());
      //  // Unstable assertion: assertNotNull(evaluation0);
      
      String string0 = evaluation0.toSummaryString();
      //  // Unstable assertion: assertEquals("\nTotal Number of Instances                0     \n", string0);
      //  // Unstable assertion: assertEquals((-2), testInstances0.NO_CLASS);
      //  // Unstable assertion: assertEquals((-1), testInstances0.CLASS_IS_LAST);
      //  // Unstable assertion: assertEquals(2, sGD0.SQUAREDLOSS);
      //  // Unstable assertion: assertEquals(0, sGD0.HINGE);
      //  // Unstable assertion: assertEquals(1, sGD0.LOGLOSS);
      //  // Unstable assertion: assertEquals(3, testInstances0.getNumAttributes());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumRelational());
      //  // Unstable assertion: assertEquals(2, testInstances0.getNumClasses());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumNumeric());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumString());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumRelationalDate());
      //  // Unstable assertion: assertEquals(1, testInstances0.getClassType());
      //  // Unstable assertion: assertEquals(-1, testInstances0.getClassIndex());
      //  // Unstable assertion: assertFalse(testInstances0.getNoClass());
      //  // Unstable assertion: assertEquals(2, testInstances0.getNumNominalValues());
      //  // Unstable assertion: assertEquals(10, testInstances0.getNumInstancesRelational());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumDate());
      //  // Unstable assertion: assertEquals(2, testInstances0.getNumRelationalNominalValues());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumRelationalNumeric());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumRelationalString());
      //  // Unstable assertion: assertEquals("Testdata", testInstances0.getRelation());
      //  // Unstable assertion: assertEquals(20, testInstances0.getNumInstances());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumNominal());
      //  // Unstable assertion: assertEquals(" ", testInstances0.getWordSeparators());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumRelationalNominal());
      //  // Unstable assertion: assertFalse(testInstances0.getMultiInstance());
      //  // Unstable assertion: assertEquals(1, testInstances0.getSeed());
      //  // Unstable assertion: assertEquals(0, capabilities0.getMinimumNumberInstances());
      //  // Unstable assertion: assertFalse(capabilities0.hasDependencies());
      //  // Unstable assertion: assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      //  // Unstable assertion: assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      //  // Unstable assertion: assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      //  // Unstable assertion: assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      //  // Unstable assertion: assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      //  // Unstable assertion: assertEquals(1, sGD0.getSeed());
      //  // Unstable assertion: assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      //  // Unstable assertion: assertFalse(sGD0.getDontReplaceMissing());
      //  // Unstable assertion: assertEquals("The random number seed to be used.", sGD0.seedTipText());
      //  // Unstable assertion: assertFalse(sGD0.getDontNormalize());
      //  // Unstable assertion: assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      //  // Unstable assertion: assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      //  // Unstable assertion: assertEquals(500, sGD0.getEpochs());
      //  // Unstable assertion: assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      //  // Unstable assertion: assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      //  // Unstable assertion: assertFalse(sGD0.getDebug());
      //  // Unstable assertion: assertEquals(1.0, evaluation0.kappa(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      //  // Unstable assertion: assertFalse(evaluation0.getDiscardPredictions());
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.correct(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      //  // Unstable assertion: assertFalse(instances0.checkForStringAttributes());
      //  // Unstable assertion: assertEquals(2, instances0.numClasses());
      //  // Unstable assertion: assertEquals(20, instances0.size());
      //  // Unstable assertion: assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      //  // Unstable assertion: assertEquals("Testdata", instances0.relationName());
      //  // Unstable assertion: assertEquals(3, instances0.numAttributes());
      //  // Unstable assertion: assertEquals(20, instances0.numInstances());
      //  // Unstable assertion: assertEquals(2, instances0.classIndex());
      //  // Unstable assertion: assertNotNull(string0);
      
      double double0 = evaluation0.relativeAbsoluteError();
      //  // Unstable assertion: assertEquals(Double.NaN, double0, 0.01D);
      //  // Unstable assertion: assertEquals((-2), testInstances0.NO_CLASS);
      //  // Unstable assertion: assertEquals((-1), testInstances0.CLASS_IS_LAST);
      //  // Unstable assertion: assertEquals(2, sGD0.SQUAREDLOSS);
      //  // Unstable assertion: assertEquals(0, sGD0.HINGE);
      //  // Unstable assertion: assertEquals(1, sGD0.LOGLOSS);
      //  // Unstable assertion: assertEquals(3, testInstances0.getNumAttributes());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumRelational());
      //  // Unstable assertion: assertEquals(2, testInstances0.getNumClasses());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumNumeric());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumString());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumRelationalDate());
      //  // Unstable assertion: assertEquals(1, testInstances0.getClassType());
      //  // Unstable assertion: assertEquals(-1, testInstances0.getClassIndex());
      //  // Unstable assertion: assertFalse(testInstances0.getNoClass());
      //  // Unstable assertion: assertEquals(2, testInstances0.getNumNominalValues());
      //  // Unstable assertion: assertEquals(10, testInstances0.getNumInstancesRelational());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumDate());
      //  // Unstable assertion: assertEquals(2, testInstances0.getNumRelationalNominalValues());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumRelationalNumeric());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumRelationalString());
      //  // Unstable assertion: assertEquals("Testdata", testInstances0.getRelation());
      //  // Unstable assertion: assertEquals(20, testInstances0.getNumInstances());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumNominal());
      //  // Unstable assertion: assertEquals(" ", testInstances0.getWordSeparators());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumRelationalNominal());
      //  // Unstable assertion: assertFalse(testInstances0.getMultiInstance());
      //  // Unstable assertion: assertEquals(1, testInstances0.getSeed());
      //  // Unstable assertion: assertEquals(0, capabilities0.getMinimumNumberInstances());
      //  // Unstable assertion: assertFalse(capabilities0.hasDependencies());
      //  // Unstable assertion: assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      //  // Unstable assertion: assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      //  // Unstable assertion: assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      //  // Unstable assertion: assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      //  // Unstable assertion: assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      //  // Unstable assertion: assertEquals(1, sGD0.getSeed());
      //  // Unstable assertion: assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      //  // Unstable assertion: assertFalse(sGD0.getDontReplaceMissing());
      //  // Unstable assertion: assertEquals("The random number seed to be used.", sGD0.seedTipText());
      //  // Unstable assertion: assertFalse(sGD0.getDontNormalize());
      //  // Unstable assertion: assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      //  // Unstable assertion: assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      //  // Unstable assertion: assertEquals(500, sGD0.getEpochs());
      //  // Unstable assertion: assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      //  // Unstable assertion: assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      //  // Unstable assertion: assertFalse(sGD0.getDebug());
      //  // Unstable assertion: assertEquals(1.0, evaluation0.kappa(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      //  // Unstable assertion: assertFalse(evaluation0.getDiscardPredictions());
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.correct(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      //  // Unstable assertion: assertFalse(instances0.checkForStringAttributes());
      //  // Unstable assertion: assertEquals(2, instances0.numClasses());
      //  // Unstable assertion: assertEquals(20, instances0.size());
      //  // Unstable assertion: assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      //  // Unstable assertion: assertEquals("Testdata", instances0.relationName());
      //  // Unstable assertion: assertEquals(3, instances0.numAttributes());
      //  // Unstable assertion: assertEquals(20, instances0.numInstances());
      //  // Unstable assertion: assertEquals(2, instances0.classIndex());
      
      double double1 = evaluation0.weightedAreaUnderPRC();
      //  // Unstable assertion: assertTrue(double1 == double0);
      //  // Unstable assertion: assertEquals(Double.NaN, double1, 0.01D);
      //  // Unstable assertion: assertEquals((-2), testInstances0.NO_CLASS);
      //  // Unstable assertion: assertEquals((-1), testInstances0.CLASS_IS_LAST);
      //  // Unstable assertion: assertEquals(2, sGD0.SQUAREDLOSS);
      //  // Unstable assertion: assertEquals(0, sGD0.HINGE);
      //  // Unstable assertion: assertEquals(1, sGD0.LOGLOSS);
      //  // Unstable assertion: assertEquals(3, testInstances0.getNumAttributes());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumRelational());
      //  // Unstable assertion: assertEquals(2, testInstances0.getNumClasses());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumNumeric());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumString());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumRelationalDate());
      //  // Unstable assertion: assertEquals(1, testInstances0.getClassType());
      //  // Unstable assertion: assertEquals(-1, testInstances0.getClassIndex());
      //  // Unstable assertion: assertFalse(testInstances0.getNoClass());
      //  // Unstable assertion: assertEquals(2, testInstances0.getNumNominalValues());
      //  // Unstable assertion: assertEquals(10, testInstances0.getNumInstancesRelational());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumDate());
      //  // Unstable assertion: assertEquals(2, testInstances0.getNumRelationalNominalValues());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumRelationalNumeric());
      //  // Unstable assertion: assertEquals(0, testInstances0.getNumRelationalString());
      //  // Unstable assertion: assertEquals("Testdata", testInstances0.getRelation());
      //  // Unstable assertion: assertEquals(20, testInstances0.getNumInstances());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumNominal());
      //  // Unstable assertion: assertEquals(" ", testInstances0.getWordSeparators());
      //  // Unstable assertion: assertEquals(1, testInstances0.getNumRelationalNominal());
      //  // Unstable assertion: assertFalse(testInstances0.getMultiInstance());
      //  // Unstable assertion: assertEquals(1, testInstances0.getSeed());
      //  // Unstable assertion: assertEquals(0, capabilities0.getMinimumNumberInstances());
      //  // Unstable assertion: assertFalse(capabilities0.hasDependencies());
      //  // Unstable assertion: assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      //  // Unstable assertion: assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      //  // Unstable assertion: assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      //  // Unstable assertion: assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      //  // Unstable assertion: assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      //  // Unstable assertion: assertEquals(1, sGD0.getSeed());
      //  // Unstable assertion: assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      //  // Unstable assertion: assertFalse(sGD0.getDontReplaceMissing());
      //  // Unstable assertion: assertEquals("The random number seed to be used.", sGD0.seedTipText());
      //  // Unstable assertion: assertFalse(sGD0.getDontNormalize());
      //  // Unstable assertion: assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      //  // Unstable assertion: assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      //  // Unstable assertion: assertEquals(500, sGD0.getEpochs());
      //  // Unstable assertion: assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      //  // Unstable assertion: assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      //  // Unstable assertion: assertFalse(sGD0.getDebug());
      //  // Unstable assertion: assertEquals(1.0, evaluation0.kappa(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      //  // Unstable assertion: assertFalse(evaluation0.getDiscardPredictions());
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.correct(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      //  // Unstable assertion: assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      //  // Unstable assertion: assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      //  // Unstable assertion: assertFalse(instances0.checkForStringAttributes());
      //  // Unstable assertion: assertEquals(2, instances0.numClasses());
      //  // Unstable assertion: assertEquals(20, instances0.size());
      //  // Unstable assertion: assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      //  // Unstable assertion: assertEquals("Testdata", instances0.relationName());
      //  // Unstable assertion: assertEquals(3, instances0.numAttributes());
      //  // Unstable assertion: assertEquals(20, instances0.numInstances());
      //  // Unstable assertion: assertEquals(2, instances0.classIndex());
  }

  //Test case number: 15
  /*
   * 6 covered goals:
   * Goal 1. weka.classifiers.Evaluation.toMatrixString()Ljava/lang/String;: root-Branch
   * Goal 2. weka.classifiers.Evaluation.toMatrixString(Ljava/lang/String;)Ljava/lang/String;: I126 Branch 239 IFNE L2640 - true
   * Goal 3. weka.classifiers.Evaluation.toMatrixString(Ljava/lang/String;)Ljava/lang/String;: I162 Branch 240 IF_ICMPGE L2647 - true
   * Goal 4. weka.classifiers.Evaluation.toMatrixString(Ljava/lang/String;)Ljava/lang/String;: I267 Branch 246 IFEQ L2663 - true
   * Goal 5. weka.classifiers.Evaluation.toMatrixString(Ljava/lang/String;)Ljava/lang/String;: I311 Branch 247 IF_ICMPGE L2667 - true
   * Goal 6. weka.classifiers.Evaluation.toMatrixString(Ljava/lang/String;)Ljava/lang/String;: I375 Branch 249 IF_ICMPGE L2676 - true
   */

  @Test
  public void test15()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertNotNull(textDirectoryLoader0);
      
      Instances instances0 = textDirectoryLoader0.getStructure();
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertNotNull(evaluation0);
      
      String string0 = evaluation0.toMatrixString();
      assertEquals("=== Confusion Matrix ===\n\n   <-- classified as\n", string0);
      assertEquals(0, instances0.numClasses());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(0, instances0.numInstances());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertNotNull(string0);
  }

  //Test case number: 16
  /*
   * 1 covered goal:
   * Goal 1. weka.classifiers.Evaluation.makeOptionString(Lweka/classifiers/Classifier;Z)Ljava/lang/String;: I514 Branch 354 IFEQ L3599 - false
   */

  @Test
  public void test16()  throws Throwable  {
      String string0 = Evaluation.makeOptionString((Classifier) null, true);
      assertEquals("\n\nGeneral options:\n\n-h or -help\n\tOutput help information.\n-synopsis or -info\n\tOutput synopsis for classifier (use in conjunction  with -h)\n-t <name of training file>\n\tSets training file.\n-T <name of test file>\n\tSets test file. If missing, a cross-validation will be performed\n\ton the training data.\n-c <class index>\n\tSets index of class attribute (default: last).\n-x <number of folds>\n\tSets number of folds for cross-validation (default: 10).\n-no-cv\n\tDo not perform any cross validation.\n-split-percentage <percentage>\n\tSets the percentage for the train/test set split, e.g., 66.\n-preserve-order\n\tPreserves the order in the percentage split.\n-s <random number seed>\n\tSets random number seed for cross-validation or percentage split\n\t(default: 1).\n-m <name of file with cost matrix>\n\tSets file with cost matrix.\n-l <name of input file>\n\tSets model input file. In case the filename ends with '.xml',\n\ta PMML file is loaded or, if that fails, options are loaded\n\tfrom the XML file.\n-d <name of output file>\n\tSets model output file. In case the filename ends with '.xml',\n\tonly the options are saved to the XML file, not the model.\n-v\n\tOutputs no statistics for training data.\n-o\n\tOutputs statistics only, not the classifier.\n-i\n\tOutputs detailed information-retrieval statistics for each class.\n-k\n\tOutputs information-theoretic statistics.\n-classifications \"weka.classifiers.evaluation.output.prediction.AbstractOutput + options\"\n\tUses the specified class for generating the classification output.\n\tE.g.: weka.classifiers.evaluation.output.prediction.PlainText\n-p range\n\tOutputs predictions for test instances (or the train instances if\n\tno test instances provided and -no-cv is used), along with the \n\tattributes in the specified range (and nothing else). \n\tUse '-p 0' if no attributes are desired.\n\tDeprecated: use \"-classifications ...\" instead.\n-distribution\n\tOutputs the distribution instead of only the prediction\n\tin conjunction with the '-p' option (only nominal classes).\n\tDeprecated: use \"-classifications ...\" instead.\n-r\n\tOnly outputs cumulative margin distribution.\n-xml filename | xml-string\n\tRetrieves the options from the XML-data instead of the command line.\n-threshold-file <file>\n\tThe file to save the threshold data to.\n\tThe format is determined by the extensions, e.g., '.arff' for ARFF \n\tformat or '.csv' for CSV.\n-threshold-label <label>\n\tThe class label to determine the threshold data for\n\t(default is the first label)\n", string0);
      assertNotNull(string0);
  }

  //Test case number: 17
  /*
   * 15 covered goals:
   * Goal 1. weka.classifiers.Evaluation.trueNegativeRate(I)D: I18 Branch 269 IF_ICMPGE L2891 - true
   * Goal 2. weka.classifiers.Evaluation.trueNegativeRate(I)D: I18 Branch 269 IF_ICMPGE L2891 - false
   * Goal 3. weka.classifiers.Evaluation.trueNegativeRate(I)D: I26 Branch 270 IF_ICMPEQ L2892 - true
   * Goal 4. weka.classifiers.Evaluation.trueNegativeRate(I)D: I26 Branch 270 IF_ICMPEQ L2892 - false
   * Goal 5. weka.classifiers.Evaluation.trueNegativeRate(I)D: I38 Branch 271 IF_ICMPGE L2893 - true
   * Goal 6. weka.classifiers.Evaluation.trueNegativeRate(I)D: I38 Branch 271 IF_ICMPGE L2893 - false
   * Goal 7. weka.classifiers.Evaluation.trueNegativeRate(I)D: I46 Branch 272 IF_ICMPEQ L2894 - true
   * Goal 8. weka.classifiers.Evaluation.trueNegativeRate(I)D: I46 Branch 272 IF_ICMPEQ L2894 - false
   * Goal 9. weka.classifiers.Evaluation.trueNegativeRate(I)D: I91 Branch 273 IFNE L2901 - false
   * Goal 10. weka.classifiers.Evaluation.weightedTrueNegativeRate()D: I21 Branch 274 IF_ICMPGE L2916 - true
   * Goal 11. weka.classifiers.Evaluation.weightedTrueNegativeRate()D: I21 Branch 274 IF_ICMPGE L2916 - false
   * Goal 12. weka.classifiers.Evaluation.weightedTrueNegativeRate()D: I33 Branch 275 IF_ICMPGE L2917 - true
   * Goal 13. weka.classifiers.Evaluation.weightedTrueNegativeRate()D: I33 Branch 275 IF_ICMPGE L2917 - false
   * Goal 14. weka.classifiers.Evaluation.weightedTrueNegativeRate()D: I85 Branch 276 IF_ICMPGE L2924 - true
   * Goal 15. weka.classifiers.Evaluation.weightedTrueNegativeRate()D: I85 Branch 276 IF_ICMPGE L2924 - false
   */

  @Test
  public void test17()  throws Throwable  {
      J48 j48_0 = new J48();
      assertTrue(j48_0.getSubtreeRaising());
      assertEquals("Whether MDL correction is used when finding splits on numeric attributes.", j48_0.useMDLcorrectionTipText());
      assertEquals("The confidence factor used for pruning (smaller values incur more pruning).", j48_0.confidenceFactorTipText());
      assertEquals("Determines the amount of data used for reduced-error pruning.  One fold is used for pruning, the rest for growing the tree.", j48_0.numFoldsTipText());
      assertFalse(j48_0.getSaveInstanceData());
      assertEquals("Whether reduced-error pruning is used instead of C.4.5 pruning.", j48_0.reducedErrorPruningTipText());
      assertEquals("Whether to save the training data for visualization.", j48_0.saveInstanceDataTipText());
      assertEquals("Whether counts at leaves are smoothed based on Laplace.", j48_0.useLaplaceTipText());
      assertEquals(2, j48_0.getMinNumObj());
      assertTrue(j48_0.getCollapseTree());
      assertTrue(j48_0.getUseMDLcorrection());
      assertEquals(3, j48_0.getNumFolds());
      assertEquals("Whether to use binary splits on nominal attributes when building the trees.", j48_0.binarySplitsTipText());
      assertEquals("Whether pruning is performed.", j48_0.unprunedTipText());
      assertEquals("The seed used for randomizing the data when reduced-error pruning is used.", j48_0.seedTipText());
      assertEquals("Whether to consider the subtree raising operation when pruning.", j48_0.subtreeRaisingTipText());
      assertFalse(j48_0.getUnpruned());
      assertFalse(j48_0.getUseLaplace());
      assertFalse(j48_0.getBinarySplits());
      assertFalse(j48_0.getReducedErrorPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", j48_0.debugTipText());
      assertEquals("Whether parts are removed that do not reduce training error.", j48_0.collapseTreeTipText());
      assertEquals(0.25F, j48_0.getConfidenceFactor(), 0.01F);
      assertEquals(1, j48_0.getSeed());
      assertEquals("The minimum number of instances per leaf.", j48_0.minNumObjTipText());
      assertEquals(1, j48_0.graphType());
      assertFalse(j48_0.getDebug());
      assertNotNull(j48_0);
      
      Capabilities capabilities0 = j48_0.getCapabilities();
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(j48_0.getSubtreeRaising());
      assertEquals("Whether MDL correction is used when finding splits on numeric attributes.", j48_0.useMDLcorrectionTipText());
      assertEquals("The confidence factor used for pruning (smaller values incur more pruning).", j48_0.confidenceFactorTipText());
      assertEquals("Determines the amount of data used for reduced-error pruning.  One fold is used for pruning, the rest for growing the tree.", j48_0.numFoldsTipText());
      assertFalse(j48_0.getSaveInstanceData());
      assertEquals("Whether reduced-error pruning is used instead of C.4.5 pruning.", j48_0.reducedErrorPruningTipText());
      assertEquals("Whether to save the training data for visualization.", j48_0.saveInstanceDataTipText());
      assertEquals("Whether counts at leaves are smoothed based on Laplace.", j48_0.useLaplaceTipText());
      assertEquals(2, j48_0.getMinNumObj());
      assertTrue(j48_0.getCollapseTree());
      assertTrue(j48_0.getUseMDLcorrection());
      assertEquals(3, j48_0.getNumFolds());
      assertEquals("Whether to use binary splits on nominal attributes when building the trees.", j48_0.binarySplitsTipText());
      assertEquals("Whether pruning is performed.", j48_0.unprunedTipText());
      assertEquals("The seed used for randomizing the data when reduced-error pruning is used.", j48_0.seedTipText());
      assertEquals("Whether to consider the subtree raising operation when pruning.", j48_0.subtreeRaisingTipText());
      assertFalse(j48_0.getUnpruned());
      assertFalse(j48_0.getUseLaplace());
      assertFalse(j48_0.getBinarySplits());
      assertFalse(j48_0.getReducedErrorPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", j48_0.debugTipText());
      assertEquals("Whether parts are removed that do not reduce training error.", j48_0.collapseTreeTipText());
      assertEquals(0.25F, j48_0.getConfidenceFactor(), 0.01F);
      assertEquals(1, j48_0.getSeed());
      assertEquals("The minimum number of instances per leaf.", j48_0.minNumObjTipText());
      assertEquals(1, j48_0.graphType());
      assertFalse(j48_0.getDebug());
      assertNotNull(capabilities0);
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertTrue(j48_0.getSubtreeRaising());
      assertEquals("Whether MDL correction is used when finding splits on numeric attributes.", j48_0.useMDLcorrectionTipText());
      assertEquals("The confidence factor used for pruning (smaller values incur more pruning).", j48_0.confidenceFactorTipText());
      assertEquals("Determines the amount of data used for reduced-error pruning.  One fold is used for pruning, the rest for growing the tree.", j48_0.numFoldsTipText());
      assertFalse(j48_0.getSaveInstanceData());
      assertEquals("Whether reduced-error pruning is used instead of C.4.5 pruning.", j48_0.reducedErrorPruningTipText());
      assertEquals("Whether to save the training data for visualization.", j48_0.saveInstanceDataTipText());
      assertEquals("Whether counts at leaves are smoothed based on Laplace.", j48_0.useLaplaceTipText());
      assertEquals(2, j48_0.getMinNumObj());
      assertTrue(j48_0.getCollapseTree());
      assertTrue(j48_0.getUseMDLcorrection());
      assertEquals(3, j48_0.getNumFolds());
      assertEquals("Whether to use binary splits on nominal attributes when building the trees.", j48_0.binarySplitsTipText());
      assertEquals("Whether pruning is performed.", j48_0.unprunedTipText());
      assertEquals("The seed used for randomizing the data when reduced-error pruning is used.", j48_0.seedTipText());
      assertEquals("Whether to consider the subtree raising operation when pruning.", j48_0.subtreeRaisingTipText());
      assertFalse(j48_0.getUnpruned());
      assertFalse(j48_0.getUseLaplace());
      assertFalse(j48_0.getBinarySplits());
      assertFalse(j48_0.getReducedErrorPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", j48_0.debugTipText());
      assertEquals("Whether parts are removed that do not reduce training error.", j48_0.collapseTreeTipText());
      assertEquals(0.25F, j48_0.getConfidenceFactor(), 0.01F);
      assertEquals(1, j48_0.getSeed());
      assertEquals("The minimum number of instances per leaf.", j48_0.minNumObjTipText());
      assertEquals(1, j48_0.graphType());
      assertFalse(j48_0.getDebug());
      assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate("G8j8>eC2t8_V$wT");
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(3, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(4, instances0.numAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertTrue(j48_0.getSubtreeRaising());
      assertEquals("Whether MDL correction is used when finding splits on numeric attributes.", j48_0.useMDLcorrectionTipText());
      assertEquals("The confidence factor used for pruning (smaller values incur more pruning).", j48_0.confidenceFactorTipText());
      assertEquals("Determines the amount of data used for reduced-error pruning.  One fold is used for pruning, the rest for growing the tree.", j48_0.numFoldsTipText());
      assertFalse(j48_0.getSaveInstanceData());
      assertEquals("Whether reduced-error pruning is used instead of C.4.5 pruning.", j48_0.reducedErrorPruningTipText());
      assertEquals("Whether to save the training data for visualization.", j48_0.saveInstanceDataTipText());
      assertEquals("Whether counts at leaves are smoothed based on Laplace.", j48_0.useLaplaceTipText());
      assertEquals(2, j48_0.getMinNumObj());
      assertTrue(j48_0.getCollapseTree());
      assertTrue(j48_0.getUseMDLcorrection());
      assertEquals(3, j48_0.getNumFolds());
      assertEquals("Whether to use binary splits on nominal attributes when building the trees.", j48_0.binarySplitsTipText());
      assertEquals("Whether pruning is performed.", j48_0.unprunedTipText());
      assertEquals("The seed used for randomizing the data when reduced-error pruning is used.", j48_0.seedTipText());
      assertEquals("Whether to consider the subtree raising operation when pruning.", j48_0.subtreeRaisingTipText());
      assertFalse(j48_0.getUnpruned());
      assertFalse(j48_0.getUseLaplace());
      assertFalse(j48_0.getBinarySplits());
      assertFalse(j48_0.getReducedErrorPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", j48_0.debugTipText());
      assertEquals("Whether parts are removed that do not reduce training error.", j48_0.collapseTreeTipText());
      assertEquals(0.25F, j48_0.getConfidenceFactor(), 0.01F);
      assertEquals(1, j48_0.getSeed());
      assertEquals("The minimum number of instances per leaf.", j48_0.minNumObjTipText());
      assertEquals(1, j48_0.graphType());
      assertFalse(j48_0.getDebug());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(3, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(4, instances0.numAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertTrue(j48_0.getSubtreeRaising());
      assertEquals("Whether MDL correction is used when finding splits on numeric attributes.", j48_0.useMDLcorrectionTipText());
      assertEquals("The confidence factor used for pruning (smaller values incur more pruning).", j48_0.confidenceFactorTipText());
      assertEquals("Determines the amount of data used for reduced-error pruning.  One fold is used for pruning, the rest for growing the tree.", j48_0.numFoldsTipText());
      assertFalse(j48_0.getSaveInstanceData());
      assertEquals("Whether reduced-error pruning is used instead of C.4.5 pruning.", j48_0.reducedErrorPruningTipText());
      assertEquals("Whether to save the training data for visualization.", j48_0.saveInstanceDataTipText());
      assertEquals("Whether counts at leaves are smoothed based on Laplace.", j48_0.useLaplaceTipText());
      assertEquals(2, j48_0.getMinNumObj());
      assertTrue(j48_0.getCollapseTree());
      assertTrue(j48_0.getUseMDLcorrection());
      assertEquals(3, j48_0.getNumFolds());
      assertEquals("Whether to use binary splits on nominal attributes when building the trees.", j48_0.binarySplitsTipText());
      assertEquals("Whether pruning is performed.", j48_0.unprunedTipText());
      assertEquals("The seed used for randomizing the data when reduced-error pruning is used.", j48_0.seedTipText());
      assertEquals("Whether to consider the subtree raising operation when pruning.", j48_0.subtreeRaisingTipText());
      assertFalse(j48_0.getUnpruned());
      assertFalse(j48_0.getUseLaplace());
      assertFalse(j48_0.getBinarySplits());
      assertFalse(j48_0.getReducedErrorPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", j48_0.debugTipText());
      assertEquals("Whether parts are removed that do not reduce training error.", j48_0.collapseTreeTipText());
      assertEquals(0.25F, j48_0.getConfidenceFactor(), 0.01F);
      assertEquals(1, j48_0.getSeed());
      assertEquals("The minimum number of instances per leaf.", j48_0.minNumObjTipText());
      assertEquals(1, j48_0.graphType());
      assertFalse(j48_0.getDebug());
      assertNotNull(evaluation0);
      
      double double0 = evaluation0.weightedTrueNegativeRate();
      assertEquals(Double.NaN, double0, 0.01D);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(3, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(4, instances0.numAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(4, testInstances0.getNumAttributes());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertTrue(j48_0.getSubtreeRaising());
      assertEquals("Whether MDL correction is used when finding splits on numeric attributes.", j48_0.useMDLcorrectionTipText());
      assertEquals("The confidence factor used for pruning (smaller values incur more pruning).", j48_0.confidenceFactorTipText());
      assertEquals("Determines the amount of data used for reduced-error pruning.  One fold is used for pruning, the rest for growing the tree.", j48_0.numFoldsTipText());
      assertFalse(j48_0.getSaveInstanceData());
      assertEquals("Whether reduced-error pruning is used instead of C.4.5 pruning.", j48_0.reducedErrorPruningTipText());
      assertEquals("Whether to save the training data for visualization.", j48_0.saveInstanceDataTipText());
      assertEquals("Whether counts at leaves are smoothed based on Laplace.", j48_0.useLaplaceTipText());
      assertEquals(2, j48_0.getMinNumObj());
      assertTrue(j48_0.getCollapseTree());
      assertTrue(j48_0.getUseMDLcorrection());
      assertEquals(3, j48_0.getNumFolds());
      assertEquals("Whether to use binary splits on nominal attributes when building the trees.", j48_0.binarySplitsTipText());
      assertEquals("Whether pruning is performed.", j48_0.unprunedTipText());
      assertEquals("The seed used for randomizing the data when reduced-error pruning is used.", j48_0.seedTipText());
      assertEquals("Whether to consider the subtree raising operation when pruning.", j48_0.subtreeRaisingTipText());
      assertFalse(j48_0.getUnpruned());
      assertFalse(j48_0.getUseLaplace());
      assertFalse(j48_0.getBinarySplits());
      assertFalse(j48_0.getReducedErrorPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", j48_0.debugTipText());
      assertEquals("Whether parts are removed that do not reduce training error.", j48_0.collapseTreeTipText());
      assertEquals(0.25F, j48_0.getConfidenceFactor(), 0.01F);
      assertEquals(1, j48_0.getSeed());
      assertEquals("The minimum number of instances per leaf.", j48_0.minNumObjTipText());
      assertEquals(1, j48_0.graphType());
      assertFalse(j48_0.getDebug());
  }

  //Test case number: 18
  /*
   * 14 covered goals:
   * Goal 1. weka.classifiers.Evaluation.evaluateModel(Lweka/classifiers/Classifier;[Ljava/lang/String;)Ljava/lang/String;: I144 Branch 23 IFNE L963 - false
   * Goal 2. weka.classifiers.Evaluation.evaluateModel(Lweka/classifiers/Classifier;[Ljava/lang/String;)Ljava/lang/String;: I151 Branch 24 IFEQ L963 - true
   * Goal 3. weka.classifiers.Evaluation.evaluateModel(Lweka/classifiers/Classifier;[Ljava/lang/String;)Ljava/lang/String;: I221 Branch 27 IFGT L976 - true
   * Goal 4. weka.classifiers.Evaluation.evaluateModel(Lweka/classifiers/Classifier;[Ljava/lang/String;)Ljava/lang/String;: I247 Branch 28 IF_ICMPGE L981 - true
   * Goal 5. weka.classifiers.Evaluation.evaluateModel(Lweka/classifiers/Classifier;[Ljava/lang/String;)Ljava/lang/String;: I247 Branch 28 IF_ICMPGE L981 - false
   * Goal 6. weka.classifiers.Evaluation.evaluateModel(Lweka/classifiers/Classifier;[Ljava/lang/String;)Ljava/lang/String;: I277 Branch 29 IFLE L986 - true
   * Goal 7. weka.classifiers.Evaluation.evaluateModel(Lweka/classifiers/Classifier;[Ljava/lang/String;)Ljava/lang/String;: I394 Branch 32 IFEQ L1017 - true
   * Goal 8. weka.classifiers.Evaluation.evaluateModel(Lweka/classifiers/Classifier;[Ljava/lang/String;)Ljava/lang/String;: I471 Branch 35 IFEQ L1030 - true
   * Goal 9. weka.classifiers.Evaluation.evaluateModel(Lweka/classifiers/Classifier;[Ljava/lang/String;)Ljava/lang/String;: I490 Branch 36 IFEQ L1034 - true
   * Goal 10. weka.classifiers.Evaluation.evaluateModel(Lweka/classifiers/Classifier;[Ljava/lang/String;)Ljava/lang/String;: I503 Branch 37 IFNE L1037 - false
   * Goal 11. weka.classifiers.Evaluation.evaluateModel(Lweka/classifiers/Classifier;[Ljava/lang/String;)Ljava/lang/String;: I511 Branch 38 IFNE L1038 - false
   * Goal 12. weka.classifiers.Evaluation.makeOptionString(Lweka/classifiers/Classifier;Z)Ljava/lang/String;: I436 Branch 352 IFEQ L3587 - false
   * Goal 13. weka.classifiers.Evaluation.makeOptionString(Lweka/classifiers/Classifier;Z)Ljava/lang/String;: I467 Branch 353 IFEQ L3591 - true
   * Goal 14. weka.classifiers.Evaluation.makeOptionString(Lweka/classifiers/Classifier;Z)Ljava/lang/String;: I467 Branch 353 IFEQ L3591 - false
   */

  @Test
  public void test18()  throws Throwable  {
      AdditiveRegression additiveRegression0 = new AdditiveRegression();
      assertEquals("The number of iterations to be performed.", additiveRegression0.numIterationsTipText());
      assertEquals("The base classifier to be used.", additiveRegression0.classifierTipText());
      assertFalse(additiveRegression0.getDebug());
      assertEquals(0.0, additiveRegression0.measureNumIterations(), 0.01D);
      assertEquals(10, additiveRegression0.getNumIterations());
      assertEquals(1.0, additiveRegression0.getShrinkage(), 0.01D);
      assertEquals("Shrinkage rate. Smaller values help prevent overfitting and have a smoothing effect (but increase learning time). Default = 1.0, ie. no shrinkage.", additiveRegression0.shrinkageTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", additiveRegression0.debugTipText());
      assertNotNull(additiveRegression0);
      
      String[] stringArray0 = new String[2];
      stringArray0[0] = "Fatal error.";
      stringArray0[1] = "Fatal error.";
      try {
        String string0 = Evaluation.evaluateModel((Classifier) additiveRegression0, stringArray0);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // 
         // Weka exception: No training file and no object input file given.
         // 
         // General options:
         // 
         // -h or -help
         // \tOutput help information.
         // -synopsis or -info
         // \tOutput synopsis for classifier (use in conjunction  with -h)
         // -t <name of training file>
         // \tSets training file.
         // -T <name of test file>
         // \tSets test file. If missing, a cross-validation will be performed
         // \ton the training data.
         // -c <class index>
         // \tSets index of class attribute (default: last).
         // -x <number of folds>
         // \tSets number of folds for cross-validation (default: 10).
         // -no-cv
         // \tDo not perform any cross validation.
         // -split-percentage <percentage>
         // \tSets the percentage for the train/test set split, e.g., 66.
         // -preserve-order
         // \tPreserves the order in the percentage split.
         // -s <random number seed>
         // \tSets random number seed for cross-validation or percentage split
         // \t(default: 1).
         // -m <name of file with cost matrix>
         // \tSets file with cost matrix.
         // -l <name of input file>
         // \tSets model input file. In case the filename ends with '.xml',
         // \ta PMML file is loaded or, if that fails, options are loaded
         // \tfrom the XML file.
         // -d <name of output file>
         // \tSets model output file. In case the filename ends with '.xml',
         // \tonly the options are saved to the XML file, not the model.
         // -v
         // \tOutputs no statistics for training data.
         // -o
         // \tOutputs statistics only, not the classifier.
         // -i
         // \tOutputs detailed information-retrieval statistics for each class.
         // -k
         // \tOutputs information-theoretic statistics.
         // -classifications \"weka.classifiers.evaluation.output.prediction.AbstractOutput + options\"
         // \tUses the specified class for generating the classification output.
         // \tE.g.: weka.classifiers.evaluation.output.prediction.PlainText
         // -p range
         // \tOutputs predictions for test instances (or the train instances if
         // \tno test instances provided and -no-cv is used), along with the 
         // \tattributes in the specified range (and nothing else). 
         // \tUse '-p 0' if no attributes are desired.
         // \tDeprecated: use \"-classifications ...\" instead.
         // -distribution
         // \tOutputs the distribution instead of only the prediction
         // \tin conjunction with the '-p' option (only nominal classes).
         // \tDeprecated: use \"-classifications ...\" instead.
         // -r
         // \tOnly outputs cumulative margin distribution.
         // -xml filename | xml-string
         // \tRetrieves the options from the XML-data instead of the command line.
         // -threshold-file <file>
         // \tThe file to save the threshold data to.
         // \tThe format is determined by the extensions, e.g., '.arff' for ARFF 
         // \tformat or '.csv' for CSV.
         // -threshold-label <label>
         // \tThe class label to determine the threshold data for
         // \t(default is the first label)
         // 
         // Options specific to weka.classifiers.meta.AdditiveRegression:
         // 
         // -S
         // \tSpecify shrinkage rate. (default = 1.0, ie. no shrinkage)
         // 
         // -I <num>
         // \tNumber of iterations.
         // \t(default 10)
         // -D
         // \tIf set, classifier is run in debug mode and
         // \tmay output additional info to the console
         // -W
         // \tFull name of base classifier.
         // \t(default: weka.classifiers.trees.DecisionStump)
         // 
         // Options specific to classifier weka.classifiers.trees.DecisionStump:
         // 
         // -D
         // \tIf set, classifier is run in debug mode and
         // \tmay output additional info to the console
         //
      }
  }

  //Test case number: 19
  /*
   * 5 covered goals:
   * Goal 1. weka.classifiers.Evaluation.numInstances()D: root-Branch
   * Goal 2. weka.classifiers.Evaluation.pctUnclassified()D: root-Branch
   * Goal 3. weka.classifiers.Evaluation.<init>(Lweka/core/Instances;Lweka/classifiers/CostMatrix;)V: I116 Branch 3 IFNULL L418 - false
   * Goal 4. weka.classifiers.Evaluation.<init>(Lweka/core/Instances;Lweka/classifiers/CostMatrix;)V: I124 Branch 4 IFNE L419 - true
   * Goal 5. weka.classifiers.Evaluation.<init>(Lweka/core/Instances;Lweka/classifiers/CostMatrix;)V: I155 Branch 5 IF_ICMPEQ L422 - false
   */

  @Test
  public void test19()  throws Throwable  {
      SGD sGD0 = new SGD();
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(0, sGD0.HINGE);
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertFalse(sGD0.getDebug());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals(1, sGD0.getSeed());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals(500, sGD0.getEpochs());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertFalse(sGD0.getDontNormalize());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertNotNull(sGD0);
      
      Capabilities capabilities0 = sGD0.getCapabilities();
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(0, sGD0.HINGE);
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertFalse(sGD0.getDebug());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals(1, sGD0.getSeed());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals(500, sGD0.getEpochs());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertFalse(sGD0.getDontNormalize());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertNotNull(capabilities0);
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(0, sGD0.HINGE);
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(3, testInstances0.getNumAttributes());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertFalse(sGD0.getDebug());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals(1, sGD0.getSeed());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals(500, sGD0.getEpochs());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertFalse(sGD0.getDontNormalize());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate("wOn{|th@`)");
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(0, sGD0.HINGE);
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(3, testInstances0.getNumAttributes());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(3, instances0.numAttributes());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertFalse(sGD0.getDebug());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals(1, sGD0.getSeed());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals(500, sGD0.getEpochs());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertFalse(sGD0.getDontNormalize());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(0, sGD0.HINGE);
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(3, testInstances0.getNumAttributes());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(3, instances0.numAttributes());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertFalse(sGD0.getDebug());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals(1, sGD0.getSeed());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals(500, sGD0.getEpochs());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertFalse(sGD0.getDontNormalize());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertNotNull(evaluation0);
      
      double double0 = evaluation0.numInstances();
      assertEquals(0.0, double0, 0.01D);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(0, sGD0.HINGE);
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(3, testInstances0.getNumAttributes());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(3, instances0.numAttributes());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertFalse(sGD0.getDebug());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals(1, sGD0.getSeed());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals(500, sGD0.getEpochs());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertFalse(sGD0.getDontNormalize());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      
      double double1 = evaluation0.pctUnclassified();
      assertFalse(double1 == double0);
      assertEquals(Double.NaN, double1, 0.01D);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(0, sGD0.HINGE);
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(3, testInstances0.getNumAttributes());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(3, instances0.numAttributes());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertFalse(sGD0.getDebug());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals(1, sGD0.getSeed());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals(500, sGD0.getEpochs());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertFalse(sGD0.getDontNormalize());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      assertEquals(1, costSensitiveClassifier0.MATRIX_ON_DEMAND);
      assertEquals(2, costSensitiveClassifier0.MATRIX_SUPPLIED);
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertNotNull(costSensitiveClassifier0);
      
      CostMatrix costMatrix0 = costSensitiveClassifier0.getCostMatrix();
      assertEquals(1, costSensitiveClassifier0.MATRIX_ON_DEMAND);
      assertEquals(2, costSensitiveClassifier0.MATRIX_SUPPLIED);
      assertEquals(1, costMatrix0.size());
      assertEquals(1, costMatrix0.numColumns());
      assertEquals(1, costMatrix0.numRows());
      assertEquals("The base classifier to be used.", costSensitiveClassifier0.classifierTipText());
      assertEquals(0, costSensitiveClassifier0.graphType());
      assertEquals("Sets the directory where cost files are loaded from. This option is used when the costMatrixSource is set to \"On Demand\".", costSensitiveClassifier0.onDemandDirectoryTipText());
      assertEquals("A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.", costSensitiveClassifier0.globalInfo());
      assertFalse(costSensitiveClassifier0.getDebug());
      assertEquals("Sets whether the minimum expected cost criteria will be used. If this is false, the training data will be reweighted according to the costs assigned to each class. If true, the minimum expected cost criteria will be used.", costSensitiveClassifier0.minimizeExpectedCostTipText());
      assertEquals("Sets the cost matrix explicitly. This matrix is used if the costMatrixSource property is set to \"Supplied\".", costSensitiveClassifier0.costMatrixTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", costSensitiveClassifier0.debugTipText());
      assertEquals("The random number seed to be used.", costSensitiveClassifier0.seedTipText());
      assertFalse(costSensitiveClassifier0.getMinimizeExpectedCost());
      assertEquals(1, costSensitiveClassifier0.getSeed());
      assertNotNull(costMatrix0);
      
      Evaluation evaluation1 = null;
      try {
        evaluation1 = new Evaluation(instances0, costMatrix0);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // Cost matrix not compatible with data!
         //
      }
  }

  //Test case number: 20
  /*
   * 1 covered goal:
   * Goal 1. weka.classifiers.Evaluation.evaluationForSingleInstance([DLweka/core/Instance;Z)D: I25 Branch 174 IFGT L1686 - false
   */

  @Test
  public void test20()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertNotNull(textDirectoryLoader0);
      
      Instances instances0 = textDirectoryLoader0.getDataSet();
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertNotNull(evaluation0);
      
      double[] doubleArray0 = new double[1];
      DenseInstance denseInstance0 = new DenseInstance(4808);
      assertEquals(6, denseInstance0.s_numericAfterDecimalPoint);
      assertEquals(1.0, denseInstance0.weight(), 0.01D);
      assertEquals(4808, denseInstance0.numAttributes());
      assertEquals(4808, denseInstance0.numValues());
      assertNotNull(denseInstance0);
      
      try {
        double double0 = evaluation0.evaluationForSingleInstance(doubleArray0, (Instance) denseInstance0, false);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
      }
  }

  //Test case number: 21
  /*
   * 4 covered goals:
   * Goal 1. weka.classifiers.Evaluation.num2ShortID(I[CI)Ljava/lang/String;: I16 Branch 357 IFLT L3651 - false
   * Goal 2. weka.classifiers.Evaluation.num2ShortID(I[CI)Ljava/lang/String;: I43 Branch 358 IFGE L3654 - true
   * Goal 3. weka.classifiers.Evaluation.num2ShortID(I[CI)Ljava/lang/String;: I43 Branch 358 IFGE L3654 - false
   * Goal 4. weka.classifiers.Evaluation.num2ShortID(I[CI)Ljava/lang/String;: I65 Branch 359 IFLT L3658 - false
   */

  @Test
  public void test21()  throws Throwable  {
      SGD sGD0 = new SGD();
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(0, sGD0.HINGE);
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals(500, sGD0.getEpochs());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertFalse(sGD0.getDontNormalize());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertFalse(sGD0.getDontReplaceMissing());
      assertFalse(sGD0.getDebug());
      assertEquals(1, sGD0.getSeed());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertNotNull(sGD0);
      
      Capabilities capabilities0 = sGD0.getCapabilities();
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(0, sGD0.HINGE);
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals(500, sGD0.getEpochs());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertFalse(sGD0.getDontNormalize());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertFalse(sGD0.getDontReplaceMissing());
      assertFalse(sGD0.getDebug());
      assertEquals(1, sGD0.getSeed());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertNotNull(capabilities0);
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(0, sGD0.HINGE);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals(500, sGD0.getEpochs());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertFalse(sGD0.getDontNormalize());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertFalse(sGD0.getDontReplaceMissing());
      assertFalse(sGD0.getDebug());
      assertEquals(1, sGD0.getSeed());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals(3, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumString());
      assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate("wOn{|th@`)");
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(0, sGD0.HINGE);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals(500, sGD0.getEpochs());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertFalse(sGD0.getDontNormalize());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertFalse(sGD0.getDontReplaceMissing());
      assertFalse(sGD0.getDebug());
      assertEquals(1, sGD0.getSeed());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals(3, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(3, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(0, sGD0.HINGE);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals(500, sGD0.getEpochs());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertFalse(sGD0.getDontNormalize());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertFalse(sGD0.getDontReplaceMissing());
      assertFalse(sGD0.getDebug());
      assertEquals(1, sGD0.getSeed());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals(3, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(3, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertNotNull(evaluation0);
      
      char[] charArray0 = new char[5];
      charArray0[2] = 'g';
      String string0 = evaluation0.num2ShortID('g', charArray0, 'g');
      assertEquals("                                                                                                    g\u0000\u0000", string0);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(0, sGD0.HINGE);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals(500, sGD0.getEpochs());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertFalse(sGD0.getDontNormalize());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertFalse(sGD0.getDontReplaceMissing());
      assertFalse(sGD0.getDebug());
      assertEquals(1, sGD0.getSeed());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals(3, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(3, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertArrayEquals(new char[] {'\u0000', '\u0000', 'g', '\u0000', '\u0000'}, charArray0);
      assertNotNull(string0);
  }

  //Test case number: 22
  /*
   * 2 covered goals:
   * Goal 1. weka.classifiers.Evaluation.num2ShortID(I[CI)Ljava/lang/String;: I16 Branch 357 IFLT L3651 - true
   * Goal 2. weka.classifiers.Evaluation.num2ShortID(I[CI)Ljava/lang/String;: I65 Branch 359 IFLT L3658 - true
   */

  @Test
  public void test22()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertNotNull(textDirectoryLoader0);
      
      Instances instances0 = textDirectoryLoader0.getStructure();
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals(0, instances0.numInstances());
      assertEquals(0, instances0.size());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(2, instances0.numAttributes());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals(0, instances0.numInstances());
      assertEquals(0, instances0.size());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(2, instances0.numAttributes());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertNotNull(evaluation0);
      
      char[] charArray0 = new char[18];
      String string0 = evaluation0.num2ShortID('\u0082', charArray0, 0);
      assertEquals("", string0);
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals(0, instances0.numInstances());
      assertEquals(0, instances0.size());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(2, instances0.numAttributes());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertNotNull(string0);
  }

  //Test case number: 23
  /*
   * 1 covered goal:
   * Goal 1. weka.classifiers.Evaluation.handleCostOption(Ljava/lang/String;I)Lweka/classifiers/CostMatrix;: I12 Branch 166 IFEQ L1574 - false
   */

  @Test
  public void test23()  throws Throwable  {
      try {
        CostMatrix costMatrix0 = Evaluation.handleCostOption("Average Cst                   3 !", 16831);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // Can't open file null.
         //
      }
  }

  //Test case number: 24
  /*
   * 2 covered goals:
   * Goal 1. weka.classifiers.Evaluation.evaluationForSingleInstance([DLweka/core/Instance;Z)D: I7 Branch 173 IFEQ L1684 - false
   * Goal 2. weka.classifiers.Evaluation.evaluationForSingleInstance([DLweka/core/Instance;Z)D: I25 Branch 174 IFGT L1686 - true
   */

  @Test
  public void test24()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertNotNull(textDirectoryLoader0);
      
      Instances instances0 = textDirectoryLoader0.getDataSet();
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertNotNull(instances0);
      
      DenseInstance denseInstance0 = new DenseInstance(4609);
      assertEquals(6, denseInstance0.s_numericAfterDecimalPoint);
      assertEquals(4609, denseInstance0.numAttributes());
      assertEquals(1.0, denseInstance0.weight(), 0.01D);
      assertEquals(4609, denseInstance0.numValues());
      assertNotNull(denseInstance0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertNotNull(evaluation0);
      
      double[] doubleArray0 = new double[15];
      doubleArray0[4] = (double) 6;
      try {
        double double0 = evaluation0.evaluationForSingleInstance(doubleArray0, (Instance) denseInstance0, true);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
      }
  }

  //Test case number: 25
  /*
   * 2 covered goals:
   * Goal 1. weka.classifiers.Evaluation.handleCostOption(Ljava/lang/String;I)Lweka/classifiers/CostMatrix;: I6 Branch 165 IFNULL L1574 - false
   * Goal 2. weka.classifiers.Evaluation.handleCostOption(Ljava/lang/String;I)Lweka/classifiers/CostMatrix;: I12 Branch 166 IFEQ L1574 - true
   */

  @Test
  public void test25()  throws Throwable  {
      CostMatrix costMatrix0 = Evaluation.handleCostOption("", (-724));
      assertNull(costMatrix0);
  }

  //Test case number: 26
  /*
   * 4 covered goals:
   * Goal 1. weka.classifiers.Evaluation.makeOptionString(Lweka/classifiers/Classifier;Z)Ljava/lang/String;: I360 Branch 350 IFEQ L3562 - true
   * Goal 2. weka.classifiers.Evaluation.makeOptionString(Lweka/classifiers/Classifier;Z)Ljava/lang/String;: I380 Branch 351 IFEQ L3567 - true
   * Goal 3. weka.classifiers.Evaluation.makeOptionString(Lweka/classifiers/Classifier;Z)Ljava/lang/String;: I436 Branch 352 IFEQ L3587 - true
   * Goal 4. weka.classifiers.Evaluation.makeOptionString(Lweka/classifiers/Classifier;Z)Ljava/lang/String;: I514 Branch 354 IFEQ L3599 - true
   */

  @Test
  public void test26()  throws Throwable  {
      String string0 = Evaluation.makeOptionString((Classifier) null, false);
      assertEquals("\n\nGeneral options:\n\n-h or -help\n\tOutput help information.\n-synopsis or -info\n\tOutput synopsis for classifier (use in conjunction  with -h)\n-t <name of training file>\n\tSets training file.\n-T <name of test file>\n\tSets test file. If missing, a cross-validation will be performed\n\ton the training data.\n-c <class index>\n\tSets index of class attribute (default: last).\n-x <number of folds>\n\tSets number of folds for cross-validation (default: 10).\n-no-cv\n\tDo not perform any cross validation.\n-split-percentage <percentage>\n\tSets the percentage for the train/test set split, e.g., 66.\n-preserve-order\n\tPreserves the order in the percentage split.\n-s <random number seed>\n\tSets random number seed for cross-validation or percentage split\n\t(default: 1).\n-m <name of file with cost matrix>\n\tSets file with cost matrix.\n-l <name of input file>\n\tSets model input file. In case the filename ends with '.xml',\n\ta PMML file is loaded or, if that fails, options are loaded\n\tfrom the XML file.\n-d <name of output file>\n\tSets model output file. In case the filename ends with '.xml',\n\tonly the options are saved to the XML file, not the model.\n-v\n\tOutputs no statistics for training data.\n-o\n\tOutputs statistics only, not the classifier.\n-i\n\tOutputs detailed information-retrieval statistics for each class.\n-k\n\tOutputs information-theoretic statistics.\n-classifications \"weka.classifiers.evaluation.output.prediction.AbstractOutput + options\"\n\tUses the specified class for generating the classification output.\n\tE.g.: weka.classifiers.evaluation.output.prediction.PlainText\n-p range\n\tOutputs predictions for test instances (or the train instances if\n\tno test instances provided and -no-cv is used), along with the \n\tattributes in the specified range (and nothing else). \n\tUse '-p 0' if no attributes are desired.\n\tDeprecated: use \"-classifications ...\" instead.\n-distribution\n\tOutputs the distribution instead of only the prediction\n\tin conjunction with the '-p' option (only nominal classes).\n\tDeprecated: use \"-classifications ...\" instead.\n-r\n\tOnly outputs cumulative margin distribution.\n-xml filename | xml-string\n\tRetrieves the options from the XML-data instead of the command line.\n-threshold-file <file>\n\tThe file to save the threshold data to.\n\tThe format is determined by the extensions, e.g., '.arff' for ARFF \n\tformat or '.csv' for CSV.\n-threshold-label <label>\n\tThe class label to determine the threshold data for\n\t(default is the first label)\n", string0);
      assertNotNull(string0);
  }

  //Test case number: 27
  /*
   * 2 covered goals:
   * Goal 1. weka.classifiers.Evaluation.getRevision()Ljava/lang/String;: root-Branch
   * Goal 2. weka.classifiers.Evaluation.coverageOfTestCasesByPredictedRegions()D: I7 Branch 188 IFNE L2016 - true
   */

  @Test
  public void test27()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertNotNull(textDirectoryLoader0);
      
      Instances instances0 = textDirectoryLoader0.getStructure();
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(0, instances0.size());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(0, instances0.size());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertNotNull(evaluation0);
      
      double double0 = evaluation0.coverageOfTestCasesByPredictedRegions();
      assertEquals(Double.NaN, double0, 0.01D);
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(0, instances0.size());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      
      String string0 = evaluation0.getRevision();
      assertEquals("9101", string0);
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(0, instances0.size());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertNotNull(string0);
  }

  //Test case number: 28
  /*
   * 1 covered goal:
   * Goal 1. weka.classifiers.Evaluation.wekaStaticWrapper(Lweka/classifiers/Sourcable;Ljava/lang/String;)Ljava/lang/String;: root-Branch
   */

  @Test
  public void test28()  throws Throwable  {
      SerializedInstancesLoader serializedInstancesLoader0 = new SerializedInstancesLoader();
      assertEquals(".bsi", serializedInstancesLoader0.getFileExtension());
      assertEquals("Use relative rather than absolute paths", serializedInstancesLoader0.useRelativePathTipText());
      assertEquals("Binary serialized instances", serializedInstancesLoader0.getFileDescription());
      assertFalse(serializedInstancesLoader0.getUseRelativePath());
      assertEquals("Reads a source that contains serialized Instances.", serializedInstancesLoader0.globalInfo());
      assertNotNull(serializedInstancesLoader0);
      
      OneR oneR0 = new OneR();
      assertEquals(6, oneR0.getMinBucketSize());
      assertFalse(oneR0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", oneR0.debugTipText());
      assertEquals("The minimum bucket size used for discretizing numeric attributes.", oneR0.minBucketSizeTipText());
      assertNotNull(oneR0);
      
      try {
        String string0 = Evaluation.wekaStaticWrapper((Sourcable) oneR0, ".gz");
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  //Test case number: 29
  /*
   * 16 covered goals:
   * Goal 1. weka.classifiers.Evaluation.<init>(Lweka/core/Instances;Lweka/classifiers/CostMatrix;)V: I58 Branch 1 IFEQ L410 - true
   * Goal 2. weka.classifiers.Evaluation.evaluationForSingleInstance([DLweka/core/Instance;Z)D: I7 Branch 173 IFEQ L1684 - true
   * Goal 3. weka.classifiers.Evaluation.setPriors(Lweka/core/Instances;)V: I12 Branch 319 IFNE L3335 - false
   * Goal 4. weka.classifiers.Evaluation.setPriors(Lweka/core/Instances;)V: I54 Branch 320 IF_ICMPGE L3345 - true
   * Goal 5. weka.classifiers.Evaluation.setPriors(Lweka/core/Instances;)V: I54 Branch 320 IF_ICMPGE L3345 - false
   * Goal 6. weka.classifiers.Evaluation.setPriors(Lweka/core/Instances;)V: I68 Branch 321 IFNE L3347 - false
   * Goal 7. weka.classifiers.Evaluation.setPriors(Lweka/core/Instances;)V: I105 Branch 322 IF_ICMPGE L3353 - true
   * Goal 8. weka.classifiers.Evaluation.setPriors(Lweka/core/Instances;)V: I105 Branch 322 IF_ICMPGE L3353 - false
   * Goal 9. weka.classifiers.Evaluation.setPriors(Lweka/core/Instances;)V: I115 Branch 323 IFNE L3354 - false
   * Goal 10. weka.classifiers.Evaluation.addNumericTrainClass(DD)V: I9 Branch 385 IFLE L3931 - true
   * Goal 11. weka.classifiers.Evaluation.addNumericTrainClass(DD)V: I9 Branch 385 IFLE L3931 - false
   * Goal 12. weka.classifiers.Evaluation.addNumericTrainClass(DD)V: I24 Branch 386 IFGE L3934 - true
   * Goal 13. weka.classifiers.Evaluation.addNumericTrainClass(DD)V: I24 Branch 386 IFGE L3934 - false
   * Goal 14. weka.classifiers.Evaluation.addNumericTrainClass(DD)V: I37 Branch 387 IFNONNULL L3939 - true
   * Goal 15. weka.classifiers.Evaluation.addNumericTrainClass(DD)V: I37 Branch 387 IFNONNULL L3939 - false
   * Goal 16. weka.classifiers.Evaluation.addNumericTrainClass(DD)V: I60 Branch 388 IF_ICMPNE L3943 - true
   */

  @Test
  public void test29()  throws Throwable  {
      CoverTree coverTree0 = new CoverTree();
      assertFalse(coverTree0.getMeasurePerformance());
      assertEquals("The base for the expansion constant.", coverTree0.baseTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", coverTree0.measurePerformanceTipText());
      assertEquals(1.3, coverTree0.getBase(), 0.01D);
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", coverTree0.distanceFunctionTipText());
      assertEquals(0.0, coverTree0.measureNumLeaves(), 0.01D);
      assertEquals(0.0, coverTree0.measureTreeSize(), 0.01D);
      assertEquals(0.0, coverTree0.measureMaxDepth(), 0.01D);
      assertNotNull(coverTree0);
      
      AdditiveRegression additiveRegression0 = new AdditiveRegression();
      assertEquals(10, additiveRegression0.getNumIterations());
      assertEquals(1.0, additiveRegression0.getShrinkage(), 0.01D);
      assertEquals("Shrinkage rate. Smaller values help prevent overfitting and have a smoothing effect (but increase learning time). Default = 1.0, ie. no shrinkage.", additiveRegression0.shrinkageTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", additiveRegression0.debugTipText());
      assertEquals("The base classifier to be used.", additiveRegression0.classifierTipText());
      assertFalse(additiveRegression0.getDebug());
      assertEquals(0.0, additiveRegression0.measureNumIterations(), 0.01D);
      assertEquals("The number of iterations to be performed.", additiveRegression0.numIterationsTipText());
      assertNotNull(additiveRegression0);
      
      String string0 = coverTree0.globalInfo();
      assertEquals("Class implementing the CoverTree datastructure.\nThe class is very much a translation of the c source code made available by the authors.\n\nFor more information and original source code see:\n\nAlina Beygelzimer, Sham Kakade, John Langford: Cover trees for nearest neighbor. In: ICML'06: Proceedings of the 23rd international conference on Machine learning, New York, NY, USA, 97-104, 2006.", string0);
      assertFalse(coverTree0.getMeasurePerformance());
      assertEquals("The base for the expansion constant.", coverTree0.baseTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", coverTree0.measurePerformanceTipText());
      assertEquals(1.3, coverTree0.getBase(), 0.01D);
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", coverTree0.distanceFunctionTipText());
      assertEquals(0.0, coverTree0.measureNumLeaves(), 0.01D);
      assertEquals(0.0, coverTree0.measureTreeSize(), 0.01D);
      assertEquals(0.0, coverTree0.measureMaxDepth(), 0.01D);
      assertNotNull(string0);
      
      Capabilities capabilities0 = additiveRegression0.getCapabilities();
      assertEquals(10, additiveRegression0.getNumIterations());
      assertEquals(1.0, additiveRegression0.getShrinkage(), 0.01D);
      assertEquals("Shrinkage rate. Smaller values help prevent overfitting and have a smoothing effect (but increase learning time). Default = 1.0, ie. no shrinkage.", additiveRegression0.shrinkageTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", additiveRegression0.debugTipText());
      assertEquals("The base classifier to be used.", additiveRegression0.classifierTipText());
      assertFalse(additiveRegression0.getDebug());
      assertEquals(0.0, additiveRegression0.measureNumIterations(), 0.01D);
      assertEquals("The number of iterations to be performed.", additiveRegression0.numIterationsTipText());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertNotNull(capabilities0);
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals(10, additiveRegression0.getNumIterations());
      assertEquals(1.0, additiveRegression0.getShrinkage(), 0.01D);
      assertEquals("Shrinkage rate. Smaller values help prevent overfitting and have a smoothing effect (but increase learning time). Default = 1.0, ie. no shrinkage.", additiveRegression0.shrinkageTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", additiveRegression0.debugTipText());
      assertEquals("The base classifier to be used.", additiveRegression0.classifierTipText());
      assertFalse(additiveRegression0.getDebug());
      assertEquals(0.0, additiveRegression0.measureNumIterations(), 0.01D);
      assertEquals("The number of iterations to be performed.", additiveRegression0.numIterationsTipText());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate("Class implementing the CoverTree datastructure.\nThe class is very much a translation of the c source code made available by the authors.\n\nFor more information and original source code see:\n\nAlina Beygelzimer, Sham Kakade, John Langford: Cover trees for nearest neighbor. In: ICML'06: Proceedings of the 23rd international conference on Machine learning, New York, NY, USA, 97-104, 2006.");
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals(10, additiveRegression0.getNumIterations());
      assertEquals(1.0, additiveRegression0.getShrinkage(), 0.01D);
      assertEquals("Shrinkage rate. Smaller values help prevent overfitting and have a smoothing effect (but increase learning time). Default = 1.0, ie. no shrinkage.", additiveRegression0.shrinkageTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", additiveRegression0.debugTipText());
      assertEquals("The base classifier to be used.", additiveRegression0.classifierTipText());
      assertFalse(additiveRegression0.getDebug());
      assertEquals(0.0, additiveRegression0.measureNumIterations(), 0.01D);
      assertEquals("The number of iterations to be performed.", additiveRegression0.numIterationsTipText());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.size());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(1, instances0.numClasses());
      assertEquals(3, instances0.classIndex());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals(10, additiveRegression0.getNumIterations());
      assertEquals(1.0, additiveRegression0.getShrinkage(), 0.01D);
      assertEquals("Shrinkage rate. Smaller values help prevent overfitting and have a smoothing effect (but increase learning time). Default = 1.0, ie. no shrinkage.", additiveRegression0.shrinkageTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", additiveRegression0.debugTipText());
      assertEquals("The base classifier to be used.", additiveRegression0.classifierTipText());
      assertFalse(additiveRegression0.getDebug());
      assertEquals(0.0, additiveRegression0.measureNumIterations(), 0.01D);
      assertEquals("The number of iterations to be performed.", additiveRegression0.numIterationsTipText());
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(4, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.size());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(1, instances0.numClasses());
      assertEquals(3, instances0.classIndex());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(4, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertNotNull(evaluation0);
      
      double[] doubleArray0 = new double[5];
      SparseInstance sparseInstance0 = new SparseInstance(0.0, doubleArray0);
      assertEquals(6, sparseInstance0.s_numericAfterDecimalPoint);
      assertEquals(0.0, sparseInstance0.weight(), 0.01D);
      assertEquals(0, sparseInstance0.numValues());
      assertEquals(5, sparseInstance0.numAttributes());
      assertArrayEquals(new double[] {0.0, 0.0, 0.0, 0.0, 0.0}, doubleArray0, 0.01);
      assertNotNull(sparseInstance0);
      
      try {
        double double0 = evaluation0.evaluationForSingleInstance(doubleArray0, (Instance) sparseInstance0, false);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
      }
  }

  //Test case number: 30
  /*
   * 1 covered goal:
   * Goal 1. weka.classifiers.Evaluation.useNoPriors()V: root-Branch
   */

  @Test
  public void test30()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertNotNull(textDirectoryLoader0);
      
      Instances instances0 = textDirectoryLoader0.getStructure();
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(0, instances0.numInstances());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(0, instances0.numInstances());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertNotNull(evaluation0);
      
      evaluation0.useNoPriors();
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(0, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(0, instances0.numInstances());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertFalse(textDirectoryLoader0.getDebug());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("", textDirectoryLoader0.getCharSet());
  }

  //Test case number: 31
  /*
   * 1 covered goal:
   * Goal 1. weka.classifiers.Evaluation.pctCorrect()D: root-Branch
   */

  @Test
  public void test31()  throws Throwable  {
      TextDirectoryLoader textDirectoryLoader0 = new TextDirectoryLoader();
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertNotNull(textDirectoryLoader0);
      
      Instances instances0 = textDirectoryLoader0.getDataSet();
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
      assertNotNull(evaluation0);
      
      double double0 = evaluation0.pctCorrect();
      assertEquals(Double.NaN, double0, 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(2, instances0.numAttributes());
      assertEquals(0, instances0.numInstances());
      assertEquals(0, instances0.size());
      assertEquals(0.0, instances0.sumOfWeights(), 0.01D);
      assertEquals("_Users_fabiopalomba_Documents_workspace_LunchEvosuite", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(0, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals("Whether to print additional debug information to the console.", textDirectoryLoader0.debugTipText());
      assertEquals("", textDirectoryLoader0.getCharSet());
      assertEquals("The character set to use when reading text files (eg UTF-8) - leave blank to use the default character set.", textDirectoryLoader0.charSetTipText());
      assertEquals("Whether to store the filename in an additional attribute.", textDirectoryLoader0.outputFilenameTipText());
      assertFalse(textDirectoryLoader0.getOutputFilename());
      assertEquals("Loads all text files in a directory and uses the subdirectory names as class labels. The content of the text files will be stored in a String attribute, the filename can be stored as well.", textDirectoryLoader0.globalInfo());
      assertEquals("Directories", textDirectoryLoader0.getFileDescription());
      assertFalse(textDirectoryLoader0.getDebug());
  }

  //Test case number: 32
  /*
   * 12 covered goals:
   * Goal 1. weka.classifiers.Evaluation.<init>(Lweka/core/Instances;)V: root-Branch
   * Goal 2. weka.classifiers.Evaluation.<init>(Lweka/core/Instances;Lweka/classifiers/CostMatrix;)V: I58 Branch 1 IFEQ L410 - false
   * Goal 3. weka.classifiers.Evaluation.<init>(Lweka/core/Instances;Lweka/classifiers/CostMatrix;)V: I86 Branch 2 IF_ICMPGE L413 - true
   * Goal 4. weka.classifiers.Evaluation.<init>(Lweka/core/Instances;Lweka/classifiers/CostMatrix;)V: I86 Branch 2 IF_ICMPGE L413 - false
   * Goal 5. weka.classifiers.Evaluation.<init>(Lweka/core/Instances;Lweka/classifiers/CostMatrix;)V: I116 Branch 3 IFNULL L418 - true
   * Goal 6. weka.classifiers.Evaluation.predictions()Lweka/core/FastVector;: I7 Branch 187 IFEQ L1845 - true
   * Goal 7. weka.classifiers.Evaluation.setPriors(Lweka/core/Instances;)V: I12 Branch 319 IFNE L3335 - true
   * Goal 8. weka.classifiers.Evaluation.setPriors(Lweka/core/Instances;)V: I168 Branch 324 IF_ICMPGE L3362 - true
   * Goal 9. weka.classifiers.Evaluation.setPriors(Lweka/core/Instances;)V: I168 Branch 324 IF_ICMPGE L3362 - false
   * Goal 10. weka.classifiers.Evaluation.setPriors(Lweka/core/Instances;)V: I201 Branch 325 IF_ICMPGE L3366 - true
   * Goal 11. weka.classifiers.Evaluation.setPriors(Lweka/core/Instances;)V: I201 Branch 325 IF_ICMPGE L3366 - false
   * Goal 12. weka.classifiers.Evaluation.setPriors(Lweka/core/Instances;)V: I211 Branch 326 IFNE L3367 - false
   */

  @Test
  public void test32()  throws Throwable  {
      SGD sGD0 = new SGD();
      assertEquals(0, sGD0.HINGE);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertFalse(sGD0.getDontNormalize());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertFalse(sGD0.getDebug());
      assertEquals(500, sGD0.getEpochs());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals(1, sGD0.getSeed());
      assertNotNull(sGD0);
      
      Capabilities capabilities0 = sGD0.getCapabilities();
      assertEquals(0, sGD0.HINGE);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertFalse(sGD0.getDontNormalize());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertFalse(sGD0.getDebug());
      assertEquals(500, sGD0.getEpochs());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals(1, sGD0.getSeed());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertNotNull(capabilities0);
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals(0, sGD0.HINGE);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(3, testInstances0.getNumAttributes());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(sGD0.getDontNormalize());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertFalse(sGD0.getDebug());
      assertEquals(500, sGD0.getEpochs());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals(1, sGD0.getSeed());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate("wOn{|th@`)");
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals(0, sGD0.HINGE);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(2, instances0.numClasses());
      assertEquals(3, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(20, instances0.numInstances());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(3, testInstances0.getNumAttributes());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(sGD0.getDontNormalize());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertFalse(sGD0.getDebug());
      assertEquals(500, sGD0.getEpochs());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals(1, sGD0.getSeed());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertNotNull(instances0);
      
      Evaluation evaluation0 = new Evaluation(instances0);
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals(0, sGD0.HINGE);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(2, instances0.numClasses());
      assertEquals(3, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(20, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(3, testInstances0.getNumAttributes());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(sGD0.getDontNormalize());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertFalse(sGD0.getDebug());
      assertEquals(500, sGD0.getEpochs());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals(1, sGD0.getSeed());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertNotNull(evaluation0);
      
      FastVector fastVector0 = evaluation0.predictions();
      assertEquals((-1), testInstances0.CLASS_IS_LAST);
      assertEquals((-2), testInstances0.NO_CLASS);
      assertEquals(0, sGD0.HINGE);
      assertEquals(1, sGD0.LOGLOSS);
      assertEquals(2, sGD0.SQUAREDLOSS);
      assertEquals(2, instances0.numClasses());
      assertEquals(3, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01D);
      assertEquals(20, instances0.numInstances());
      assertEquals(Double.NaN, evaluation0.unweightedMicroFmeasure(), 0.01D);
      assertEquals(0.0, evaluation0.SFEntropyGain(), 0.01D);
      assertFalse(evaluation0.getDiscardPredictions());
      assertEquals(Double.NaN, evaluation0.weightedMatthewsCorrelation(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalseNegativeRate(), 0.01D);
      assertEquals(0.0, evaluation0.correct(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanEntropyGain(), 0.01D);
      assertEquals(0.0, evaluation0.SFPriorEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.unclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootRelativeSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedRecall(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctUnclassified(), 0.01D);
      assertEquals(Double.NaN, evaluation0.relativeAbsoluteError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.rootMeanPriorSquaredError(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTruePositiveRate(), 0.01D);
      assertEquals(0.0, evaluation0.incorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanAbsoluteError(), 0.01D);
      assertEquals(1.0, evaluation0.kappa(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFalsePositiveRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctIncorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.meanPriorAbsoluteError(), 0.01D);
      assertEquals(0.0, evaluation0.totalCost(), 0.01D);
      assertEquals(Double.NaN, evaluation0.sizeOfPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedPrecision(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.errorRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.SFMeanPriorEntropy(), 0.01D);
      assertEquals(0.0, evaluation0.numInstances(), 0.01D);
      assertEquals(0.0, evaluation0.SFSchemeEntropy(), 0.01D);
      assertEquals(Double.NaN, evaluation0.pctCorrect(), 0.01D);
      assertEquals(Double.NaN, evaluation0.coverageOfTestCasesByPredictedRegions(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedFMeasure(), 0.01D);
      assertEquals(Double.NaN, evaluation0.weightedTrueNegativeRate(), 0.01D);
      assertEquals(Double.NaN, evaluation0.avgCost(), 0.01D);
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(3, testInstances0.getNumAttributes());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(-1, testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(sGD0.getDontNormalize());
      assertFalse(sGD0.getDontReplaceMissing());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGD0.epochsTipText());
      assertFalse(sGD0.getDebug());
      assertEquals(500, sGD0.getEpochs());
      assertEquals(1.0E-4, sGD0.getLambda(), 0.01D);
      assertEquals("The random number seed to be used.", sGD0.seedTipText());
      assertEquals("Turn normalization off", sGD0.dontNormalizeTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGD0.lambdaTipText());
      assertEquals("The learning rate. If normalization is turned off (as it is automatically for streaming data), thenthe default learning rate will need to be reduced (try 0.0001).", sGD0.learningRateTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGD0.debugTipText());
      assertEquals("Turn off global replacement of missing values", sGD0.dontReplaceMissingTipText());
      assertEquals(0.01, sGD0.getLearningRate(), 0.01D);
      assertEquals("Implements stochastic gradient descent for learning various linear models (binary class SVM, binary class logistic regression and linear regression). Globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes, so the coefficients in the output are based on the normalized data.\nFor numeric class attributes, the squared loss function (2) must be used.", sGD0.globalInfo());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGD0.lossFunctionTipText());
      assertEquals(1, sGD0.getSeed());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertNull(fastVector0);
  }
}
